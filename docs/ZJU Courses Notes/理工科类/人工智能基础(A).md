# 人工智能基础 A
!!! abstract "Abstract"
	作为一门只有两学分的课, 平时需要完成的任务堪比五学分的专业课; 老师是根本不专业甚至不怎么会用电脑但是喜欢 PUA 人的, 讲课是完全讲不来的, 课后作业的难度是逆天的, 作业的引导是极其糟糕的, 在 word 上提供的 python 代码缩进是被毁掉的, 跨平台是一点不做的, 课程内容是极其庞大的. 如果你没有预置必修这门课, 一定要退掉, 浪费时间浪费精力影响心情还什么都学不到. 想认认真真学人工智能千万不能选这门课.

## Chap I 人工智能起源
---
!!! quote 
	这其实是一个很有趣的知识. 人工智能的诞生早于现代程序设计语言, 当时召开达特茅斯会议时大家发现, 传统的数值计算类语言(比如 `FORTRAN` 语言)难以满足表示和操纵符号结构的需求, 据此诞生了诸如 `LISP` 等符号处理类语言. 这实际上也为后来的函数式编程和元编程的设计范式.

### 1. 1 人工智能的诞生
1956 年, 达特茅斯会议的召开标志着人工智能元年的到来.

- 通过编程和算法, 能让计算机真正的思考和理解吗?

不能. 实际上计算机的设计已经决定了纯粹的算法和编程仅仅只是特定任务下的形式化操作, 计算机不具备真正意义上的"意识".

- 算法根据特定输入按照预期进行处理, 这算是智能吗?

不是. 这只能引导计算机完成定死的任务, 或者说"完成简单的反射", 其不包含自我理解, 创造力和意图生成以及其他一系列的高阶认知能力.

- 图灵测试的结果还有意义吗?

有参考意义, 但不是唯一标准. 尤其是现代的大语言模型, 图灵测试很难区别其是否是智能.

!!! tip
	**图灵测试**指的是如果人无法在对话中区别与自己对话的是人还是机器, 那么这台机器就可以称为是"智能"的.

### 1. 2 人工智能的三大学派
- **符号主义**

人的本质是台机器, 强调逻辑与规则推理, 信息使用显示符号结构表示(比如 `LISP` 语言), 常作为逻辑推理系统.

- **行为主义**

心理过程无法被观察, 不关注逻辑的"黑箱"过程, 通过不断地强化机制(强化学习)来学习.

- **联结主义**

认知过程本质上是神经元之间的联结模式, 模拟人的神经网络结构构造人工神经网络.

### 1. 3 幸存者偏差
AI 的局限绝大多数起源于幸存者偏差现象. 我们无意识或者有意识地忽略了很多信息, 导致特例成为了普遍现象.

!!! note
	最典型的例子莫过于二战时期的飞机弹痕, 当时大家都只关注到了飞回来的飞机, 没有关注中途被击落的飞机. 如果当时没有统计学家 Abraham Wald 提出的意见, 盟军将会像 AI 一样做出错误的决策.


## Chap II 机器学习
---
### 2. 1 机器学习的定义
已知一个数据集 $S$, 对任意输入的 $x\in S$ 都存在对应的标签 $y$, 通过计算机系统寻找一个数学模型 $f$, 使得对于任意 $x$, 计算得到的$y'=f(x)$ 都尽可能的逼近于对应的 $y$.

- 每**一对** $(x,y)$ 称为一个样本, 总个数叫做**样本大小**.
- $y$ 的取值范围称为**样本空间**.
- 集合 $S$ 是样本空间中的一个随机抽样.
- 机器学习是人工智能的**子集**.

!!! note
	**非机器学习**类人工智能, 就是传统的人工智能, 基本上都是有着严格规则一种算法. 例如各类游戏 AI (Alpha-Beta 剪枝, 寻路算法).

机器学习与非机器学习 AI 的区别

| 特征   | 机器学习        | 非机器学习      |
| ---- | ----------- | ---------- |
| 先验数据 | 必须          | 非必须        |
| 数学模型 | 推理模型+训练算法   | 推理模型       |
| 模型参数 | 机器自动习得      | 人工设计       |
| 准确率  | 理论上达不到 100% | 很容易达到 100% |
| 推理   | 计算机         | 计算机        |

!!! note
	这里强调的"推理模型+训练算法"实际上是强调了机器学习类可以自行更新和加强参数, 通过推理模型根据输入的 $x$ 输出 $y'$, 而训练算法则根据 $y$ 和 $y'$ 的关系不断更新推理模型的参数. 非机器学习类在人工写好模型后就不会再发生改变.

- 练习: 列出的几种算法中哪一种属于机器学习?

(1), 剪枝算法; (2), 深度优先算法; (3), 蒙特卡洛树搜索; (4), 启发式搜索; (5), 蚁群算法; (6), 宽度优先算法; (7), 最大-最小算法; (8), 线性回归

!!! success "参考答案"
	(8). 线性回归的推理模型是 $y=bx+a$, 使用最小二乘法等最小化误差的方式作为训练算法. 对于其他几项, 和机器学习类 AI 比较接近的都是启发式算法. 要么就是传统的图算法. 
	
	首先对于剪枝算法, 深度优先算法, 宽度优先算法, 最大最小搜索, 这几个都是图或者树的算法, 我们不需要进行训练, 一旦写死了代码其运行方式就不会改变.
	
	其次对于蚁群算法, 蒙特卡洛树算法和最大最小搜索算法, 虽然有了随机化模拟, 但是不会更新模型的内部参数, 因此也不属于机器学习的范畴.

- 机器学习的五个要素: 数据, 模型, 训练, 预测, 评估

实际上就是我们获得机器学习类模型的几个动作.

首先应当获取可靠的数据集, 接下来确定我们使用的模型, 而后使用数据集训练模型调整参数, 将训练后的模型用于新数据的预测, 最后由我们根据预测的准确性和其他性能评估模型.

### 2. 2 过拟合
!!! question
	训练集的结果是不是越准确越好?

!!! question
	为什么我们必须要分出训练集和测试集?

答: 不是越准确越好. 我们以最简单的线性回归模型举例, 假设存在一组数据与对应的映射

$$
X:\{1,2,3,4,5,6,7,8,9\}\longrightarrow Y:\{2,4,6,8,7,12,14,16,18\}
$$

我们很容易观察到, $Y$ 与 $X$ 的映射关系应当是 $Y=2X$, 但是当我们训练过度之后, 在 $X=5$ 的时候模型很可能输出一个接近于 $7$ 的数值. 我们称这种**在训练集上表现很好, 预测时表现很差**的现象称之为**过拟合**. 

解决过拟合问题的最佳方式就是**对收集到的数据进行预处理**, 类似于我们处理实验数据的方式, 将明显和总体趋势相悖的数据除掉, 确保训练数据的质量.

!!! quote
	**没有免费午餐定理**(NFL): 任何机器学习模型在所有问题上都是最好的. 换言之, **一个模型在某些任务上很厉害, 在另外的一些任务上一定会表现很差**. 我们只能尽可能选择合适的数据集和模型进行处理.
	
	例如, 按照上面我们解决过拟合问题的思路, 除掉 $X=5$ 对应的一组数据, 或许可以提高整体的泛化能力, 但是在这测试集组数据上的效果就没有过拟合的好.

### 2. 3 机器学习的分类

| 学习类别    | 是否存在标签 | 是否存在反馈             | 学习方式                   | 实例           |
| ------- | ------ | ------------------ | ---------------------- | ------------ |
| 监督学习    | 是      | 是(直接给出正确答案)        | 拟合输入输出关系               | 回归与分类        |
| 无监督学习   | 否      | 否                  | 机器自行寻找数据集潜在关系          | 聚类, 异常检测, 降维 |
| *半监督学习* | 是(少部分) | 是                  | 混合模式                   | -            |
| 强化学习    | 否      | 是(不知道答案, 但是存在奖励机制) | 不断试错, 不断强化思想钢印, 某种答案最优 | 决策类          |

(ps: 半监督学习了解即可)


## Chap III 无监督学习: 回归与分类模型
---
### 3. 1 常用损失函数
#### 3. 1. 1 回归损失函数
均方误差(mean squared error). 实际上就是方差.

$$
MSE=\frac{1}{N}\sum_{i=1}^N(Y_i-Y'_i)^2
$$

平均绝对误差(mean absolute error).

$$
MAE=\frac{1}{N}\sum_{i=1}^{N}\left|Y_i-Y'_i\right|
$$

平均绝对百分比误差(mean absolute percentage error).

$$
MAPE=\frac{1}{N}\sum_{i=1}^{N}\left|\frac{Y_i-Y'_i}{Y_i}\right|
$$

均方根对数误差(mean squared log error).

$$
MSLE=\frac{1}{N}\sum_{i=1}^{N}(\log \frac{Y'_i+1}{Y_i+1})^2
$$

#### 3. 1. 2 分类损失函数
!!! note
	个人认为没必要记. 太难记住了.

#### 3. 1. 3 一般终止条件
- 损失函数值足够小
- 损失函数值的变化足够小
- 训练次数足够多
- 梯度(就是模型参数变化)趋于 0
- 预测集准确率下降

### 3. 2 聚类
属于**无监督学习**, 根据数据点之间特征的**相似度**或距离(欧氏距离)将相似的数据点聚集在一起形成簇.

!!! note
	**欧氏距离**, 线性代数中的概念. 在学习线性代数时我们知道, 欧式空间是线性空间中的一种, n 维空间中的欧氏距离公式是
	
	$$
	d(X,Y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}
	$$

下面我们以典型的 **K-means Algorithm** (**K均值算法**) 为例说明聚类的过程. 

首先明确最终目的, 将数据集 $S$ 按照特征分类为 $n$ 组. 我们设 $\forall x\in S$ 都对应了 $n$ 个特征, $S$ 的大小(不同 $x$ 的个数)为 $N$. 我们将每个 $x$ 的 $n$ 个特征映射到一个 $n$ 维空间中, 于是我们得到了一个存在 $N$ 个点的空间.  

现在我们假设要将之分为 $m$ 组, 那么我们首先随机初始化 $m$ 个聚类质心, 然后不断循环下面两步:

- 计算每个数据到质心的距离, 按照如欧拉距离归类到距离最近的质心, 对应相同质心的点集我们称为簇.
- 调整质心位置, 具体做法是将当前簇内每个点距离之和的(某种)平均值最近的一处作为质心的位置.

直到:

- 达到迭代次数.
- 前后两次迭代中, 聚类的质心位置基本不变, 趋于稳定.

![](https://HollowDobt.github.io/picx-images-hosting/output-(7).6bhbzlxxng.webp)

参考演示网站: <http://alekseynp.com/viz/k-means.html>

### 3. 3 降维: 主成分分析(PCA)
目标: 在降维得到的新坐标系中实现数据波动(例如方差)的最大化.

视觉上就是建立一个新的坐标系, 将多个维度的数据投影到这个新的坐标系, 实现数据的降维. **新的坐标系轴方向就是主成分**.

在代数上, 就是将多个数据分组组合成一个新的特征, 提取最大波动的几个特征作为主特征.

![](https://HollowDobt.github.io/picx-images-hosting/output-(6).99tm343gpz.webp)


## Chap IV 深度学习(Deep Learning)导论
---

### 4. 1 多层人工神经网络与深度学习

!!! note
	深度学习**既不指监督学习或无监督学习, 也不指强化学习**. 其本身是前面几种学习类型, 也就是机器学习的实现方式.

深度学习(Deep Learning, 简称 DL)是机器学习的一个子集, 它使用**多层人工神经网络**来精准完成图像检测等任务. 通过**多层表示+高阶特征提取**完成如图像识别, 语音识别等各种任务, 具有较强的**特征鲁棒性(抗干扰)**.

![](https://HollowDobt.github.io/picx-images-hosting/output-(8).2h8kgtg188.webp)

深度学习的几个特征:

- 多层网络结构

分为**输入层, 隐藏层, 输出层**. 各层之间通过权重进行连接, 构成复杂的网络结构. 一般而言, 输入层用于"侦测和传递数据", 输出层用于将抽象信息具体化便于我们理解, 隐藏层负责提取特征进行分析.

- 自动特征提取

自动从原始数据中提取特征, 无需人为提取特征.

!!! note
	传统的机器学习需要进行人工特征提取. 比如一张图, 首先要人为提取出图片的整体纹理, 颜色直方图, 边缘提取等等, 既耗时又耗力, 而且难以泛化.
	
	深度学习通过多个隐藏层从低级特征到特征组合进行学习, 全流程自动化, 不需要人工干预.

- 非线性激活函数

深度学习模型通常使用非线性激活函数(比如 ReLU, sigmoid 等等)结合多层设计可以轻松处理复杂的非线性关系.

!!! note
	如果使用线性激活函数, 那么对于每个映射 $y=f(x)$, 都可以写作 $y=k_ix+b_i$. 假设是一个三层神经网络, 则激活函数等价于 $y=k_3(k_2(k_1x+b_1)+b_2)+b_3$, 打开合并后变成 $y=Kx+B$. 也就是说, 如果使用线性激活函数, 再多的层最后都等价于一个层.

- 大规模数据处理能力

综合前面几个优点, 尤其是自动特征提取, 使用多层人工神经网络的深度学习可以实现这一点.

### 4. 2 感知机

#### 4. 2. 1 感知机模型

这是经典的人工神经网络模型. 简单来讲, 感知机模拟了一个神经元的行为. 例如, $f$ 是一个非线性函数(就是激活函数), 而 $X$ 是一个向量矩阵. 这一模型提供一个线性函数, 将 $X$ 映射为 $\Sigma$, 其规则表述为 $\Sigma=WX^T+B$. 之后 $Y$ 作为非线性函数检测 $\Sigma$ 是否可以激活.

$$
Y=f(\Sigma),\
$$

在考试中, 会要求我们根据图写出 $Y$ 或者 $f$ (激活函数)的公式. 对于上述的映射, 我们可以形式化地表述公式为

$$
Y=f(WX^T+B)
$$

当然, 不适用矩阵也可以表达

$$
Y=f(\sum_{i=1}^{n}(w_ix_i+b))
$$

#### 4. 2. 2 多层感知机(MLP)

多层感知机(Multi-Layer Perceptron, 简称 MLP)的目的是解决非线性问题. 一个多层感知机至少包括三层: 输入层, 隐藏层, 输出层.

我们称只含有一个隐藏层的神经网络叫做**浅层学习网络**, 而将大于一个隐藏层的成为**深度学习网络**.

#### 4. 2. 3 常见激活函数
- ReLU 函数(线性整流函数), 目前(在隐藏层中)最常用

$$
f(x)=\max(0,x)
$$

- Sigmoid 函数, 其只能表述正数(ReLU 可以表述 0), 所以现在已经不常用.

$$
f(x)=\frac{1}{1+e^{-x}}
$$

- Softmax 函数, 最常用于分类问题和图像识别等, 其作用是将任意$n$ 维实数向量归一化为同阶向量.

$$
f(x_i)=\frac{e^{x_i}}{\sum_{i=1}^{n}e^{x_i}}
$$

!!! question
	对于输入 $X=[2,0.7,-1.5,-0.9]$, 计算其 Softmax 输出.

!!! success
	解:
	
	$$
	\begin{aligned}
	&sum=\sum_{i=1}^{n=4}e^{x_i}=e^2+e^{0.7}+e^{-1.5}+e^{-0.9}=10.03\\
	&S_1=\frac{e^{x_1}}{sum}=0.74\\
	&S_2=\frac{e^{x_2}}{sum}=0.20\\
	&S_3=\frac{e^{x_3}}{sum}=0.02\\
	&S_4=\frac{e^{x_4}}{sum}=0.04\\
	\end{aligned}
	$$
	
	换言之, 其 Softmax 输出为 $[0.74,0.20,0.02,0.04]$.


### 4. 3 反向传播算法: BP

反向传播是人工神经网络用于计算误差的一种手段. 简单来说, 当输出值不在损失允许范围内或根本不符合预期时, 将输出误差以某种形式通过隐藏层向输入层逐层反向传递, 反传递过程中将误差分摊到各个神经网络层, 获取各层单元的误差信号, 算法通过这些误差信号调整连接权值, 从而减小误差, 达到设计预期.

!!! note 
	反向传播算法常用于**前馈神经网络**(FNN)中. 这种神经网络最大的特点就是只会不断向下一层传递, 不像**循环神经网络**(RNN)那样可能会回到前面的隐藏层循环传递.

BP 算法的优势
- 自适应与自主学习, 可以自动化地更新隐藏层规则
- 较强的非线性映射能力
- 严谨的推导过程(链式法则)
- 较强的泛化能力(通过已有知识解决新问题)

BP 算法的劣势
- 容易陷入局部极小值(极小值不一定是最小值)
- 收敛速度缓慢(大量参数导数计算和权重与偏置值的更新)
- 隐藏层缺少理论指导, 需要不断设计隐藏层和隐藏节点数试凑达到最佳效果
- 学习新样本可能遗忘旧样本(每次更新都是按照新数据进行的, 这种情况很容易发生)

### 4. 4 梯度下降: GD
在我们**通过反向传播算法得到梯度之后**, 按照公式 $w_i\longrightarrow w_i-\eta \times \frac{\partial L}{\partial w_i}$ 不断更新本层的权重和偏置. 其中 $\eta$ 是学习率(步长).

!!! note
	可能上面的描述会让你感到困惑. 我们其实应当知道, 函数式编程的过程本质就是函数处理常用几种方法的调用. 也就是说, 我们只需要有基本的微积分知识就可以用数学描述上述过程
	
	首先, 反向传播算法本质上是对误差函数在各层偏导数的计算. 或者说, 假设某一层的激活函数为 $Y_i=f(W_iY_{i-1}^T+B_i)$, 那么反向传播就是损失函数对 $W_i$ 和 $B_i$ 求偏导的过程. 通过这一过程, 我们得到了梯度, 也就是偏导数.
	
	为了逐步追踪每个参数对于最终损失的间接影响, 我们需要使用链式法则将误差逐层从后向前传递. 比如, 对于一个两层神经网络
	
	$$
	\begin{aligned}
	&a_1=f_1(W_1x+b_1)\\
	&a_2=f_2(W_2a_1+b_2)\\
	&Loss=\varsigma (a_2,y)
	\end{aligned}
	$$
	
	其中的 $W_1$ 并不会直接影响 $Loss$ 函数. 为了计算到其间接影响, 利用多元函数微分法
	
	$$
	\frac{\partial \varsigma}{\partial W_1}=\frac{\partial \varsigma}{\partial a_2}\cdot\frac{\partial a_2}{\partial f_2}\cdot\frac{\partial f_2}{\partial a_1}\cdot\frac{\partial a_1}{\partial f_1}\cdot\frac{\partial f_1}{\partial W_1}
	$$
	
	之后的梯度下降实际上是利用反向传播计算得到的结果对所有层同时进行权重 $W$ 和偏置 $B$ 进行调整的过程.

!!! question
	设函数 $Loss=x^2$, 起点为 $(8, 64)$, 学习率为 $0.1$, 使用表格描述其梯度下降的过程.

!!! success
	(好像 admonition 里面打不了表格...)首先求导得到梯度: $y=2x$, 而后逐步计算: $y_1=2\times 8=16$, 因此更新 $x_1=x-\eta\cdot y_1=8-0.1\times 16=6.4$. 类似地, 我们进一步计算 $x_2=6.4-0.1\times (6.4\times 2)= 5,12$, ... 使用图表示为一个梯度下降的过程.
	![](https://HollowDobt.github.io/picx-images-hosting/output-(6).361u1nkymc.webp)

### 4. 5 梯度下降算法的改进: 优化器
#### 4. 5. 1 随机梯度下降法(SGD)
传统的梯度下降每次调整都要用训练集中的所有样本, 而随机梯度下降法每次只从训练集中随机选取一个或者小批量样本, 利用小规模样本训练调整神经网络.

其优势有:

- 高效
- 可并行计算
- 可适应新数据变化(预训练思想萌芽)
- 有机会全局最优

其劣势有:

- 不稳定
- 没有解决学习率选择问题(需要人工决定学习率)
- 随机最优解
- 模型不可控

改进:

- 适当增加样例
- 动量梯度下降法

相关代码
```python
# optimizer: 优化器, .parameters() 用于读取训练参数, lr 表示 learning rate
optimizer = optim.SGD(model.parameters(), lr=0.01)

# momentum: 动量, 保留上一次梯度 90% 的值, 类似于惯性
optimizer = optim.SGD(model.parameters(), momentum=0.9)
```

#### 4. 5. 2 自适应梯度算法(AdaGrad)
自适应梯度算法(AutoGrad)通过自动调整学习率实现在不同梯度尺度中稳健的表现. 其学习率的分母部分会增加历史梯度的累积值, 这意味着其学习率会随着训练次数的增加逐渐趋于 0.

其优势有:

- 自动化调整学习率
- 权重的"步调一致", 实现对典型特征(高频特征, 高学习率)的提取, 适当减少对稀疏特征(低频特征, 低学习率)的提取, 避免过拟合

其劣势有:

- 梯度消失(学习率趋于零)
- 训练速度缓慢(原因与上面一样, 学习率会逐渐减小)

!!! note
	**稀疏数据**是指维度很高但是绝大多数方向上的值是 0 的数据. 

因为 AdaGrad 的设计原理问题, 实际上的原始算法只适用于稀疏数据. 像图像等含有大量低维数据, 需要长时间进行神经网络训练的不适合 AdaGrad.

改进:
- RMSProp(自适应平方根梯度法): 调整分母的值
- Adam(自适应矩估计法, 目前最常见): 本质上是 RMSProp + Momentum

!!! note
	Adam 结合了两种优化思路, 其核心是为每个参数维护两个动量, 一个是一阶动量估计(梯度指数的加权平均), 另一个是二阶动量估计(梯度平方的指数加权平均). 如果想要详细了解, 可以看看这篇论文<https://arxiv.org/pdf/1412.6980>.

相关代码
```python
# weight_decay: 正则化项(L2 惩罚), 防止过拟合; eps 用于替代分母等于 0 的项
optimizer = optim.Adagrad(model.parameters(),
                          lr=0.01,
                          weight_decay=1e-4,
                          eps=1e-10)

# betas: 动量因子, beta1控制一阶滑动平均, beta2控制二阶平方平均
optimizer = optim.Adam(model.parameters(),
                       lr=0.01,
                       betas=(0.9, 0.999),
                       eps=1e-8,
                       weight_decay=0)
```

## 幕间: 关系总结
---

人工智能分为两大类, 一类是机器学习, 另一类是非机器学习. 前者除了模型本身, 还有训练算法, 可以不断增强自己的参数. 后者基本属于传统算法范畴, 在本门课程基本不做讨论.

对于机器学习, 我们可以从模型设计和训练算法两方面来分类. 

对于训练算法, 我们分为监督学习, 无监督学习和强化学习, 回归是典型的监督学习, 聚类和降维则是典型的无监督学习. 

对于模型设计, 我们主要讨论的是深度学习. 深度学习利用深层神经网络, 实现层级的特征抽象提取和表示学习. 现在最有名的基于 Transformer 架构的各类大模型, 就是典型的深度学习模型.

如果你是一位资深的 AI 用户, 你会发现几乎所有的 AI 都可以做到和你对话. 这本身就是件很了不起的事. 这一点归功于**预训练模型**. 预训练模型在深度学习模型的基础上, 使用大数据得到基本通用知识, 再迁移到各种具体任务中使用专业数据集进一步训练.

而当预训练模型被调教好后, 我们就得到了**生成式大模型**. 生成式大模型就是根据上下文自动生成相对高质量内容的**大规模人工神经网络**. 这其中我们最为熟悉, 最常用的便是**生成式大语言模型**.

## Chap V 深度学习实例: 卷积神经网络与循环神经网络
---
### 5. 1 CNN 卷积神经网络

卷积神经网络(CNN)是一类具有局部感受野, 权值共享机制和多层结构的前馈神经网络, 主要用于处理具有网格的数据(例如图像, 语音, 视频等), 尤其擅长从原始输入中自动提取空间和时序特征.

!!! note
	**感受野**, 指神经网络某个神经元在输入图像时可以"看到"的区域大小. 例如我们人类只能准确看到在我们实现中央的区域, 我们可以粗略的将我们的感受野归结为中间区域.
	
	在卷积神经网络中, 感受野的计算