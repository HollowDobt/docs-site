# 人工智能基础 A
!!! abstract "Abstract"
	作为一门只有两学分的课, 平时需要完成的任务堪比五学分的专业课; 老师是根本不专业甚至不怎么会用电脑但是喜欢 PUA 人的, 讲课是完全讲不来的, 课后作业的难度是逆天的, 作业的引导是极其糟糕的, 在 word 上提供的 python 代码缩进是被毁掉的, 跨平台是一点不做的, 课程内容是极其庞大的. 如果你没有预置必修这门课, 一定要退掉, 浪费时间浪费精力影响心情还什么都学不到. 想认认真真学人工智能千万不能选这门课.

## Chap I 人工智能起源
---
!!! quote 
	这其实是一个很有趣的知识. 人工智能的诞生早于现代程序设计语言, 当时召开达特茅斯会议时大家发现, 传统的数值计算类语言(比如 `FORTRAN` 语言)难以满足表示和操纵符号结构的需求, 据此诞生了诸如 `LISP` 等符号处理类语言. 这实际上也为后来的函数式编程和元编程的设计范式.

### 1. 1 人工智能的诞生
1956 年, 达特茅斯会议的召开标志着人工智能元年的到来.

- 通过编程和算法, 能让计算机真正的思考和理解吗?

不能. 实际上计算机的设计已经决定了纯粹的算法和编程仅仅只是特定任务下的形式化操作, 计算机不具备真正意义上的"意识".

- 算法根据特定输入按照预期进行处理, 这算是智能吗?

不是. 这只能引导计算机完成定死的任务, 或者说"完成简单的反射", 其不包含自我理解, 创造力和意图生成以及其他一系列的高阶认知能力.

- 图灵测试的结果还有意义吗?

有参考意义, 但不是唯一标准. 尤其是现代的大语言模型, 图灵测试很难区别其是否是智能.

!!! tip
	**图灵测试**指的是如果人无法在对话中区别与自己对话的是人还是机器, 那么这台机器就可以称为是"智能"的.

### 1. 2 人工智能的三大学派
- **符号主义**

人的本质是台机器, 强调逻辑与规则推理, 信息使用显示符号结构表示(比如 `LISP` 语言), 常作为逻辑推理系统.

- **行为主义**

心理过程无法被观察, 不关注逻辑的"黑箱"过程, 通过不断地强化机制(强化学习)来学习.

- **联结主义**

认知过程本质上是神经元之间的联结模式, 模拟人的神经网络结构构造人工神经网络.

### 1. 3 幸存者偏差
AI 的局限绝大多数起源于幸存者偏差现象. 我们无意识或者有意识地忽略了很多信息, 导致特例成为了普遍现象.

!!! note
	最典型的例子莫过于二战时期的飞机弹痕, 当时大家都只关注到了飞回来的飞机, 没有关注中途被击落的飞机. 如果当时没有统计学家 Abraham Wald 提出的意见, 盟军将会像 AI 一样做出错误的决策.


## Chap II 机器学习
---
### 2. 1 机器学习的定义
已知一个数据集 $S$, 对任意输入的 $x\in S$ 都存在对应的标签 $y$, 通过计算机系统寻找一个数学模型 $f$, 使得对于任意 $x$, 计算得到的$y'=f(x)$ 都尽可能的逼近于对应的 $y$.

- 每**一对** $(x,y)$ 称为一个样本, 总个数叫做**样本大小**.
- $y$ 的取值范围称为**样本空间**.
- 集合 $S$ 是样本空间中的一个随机抽样.
- 机器学习是人工智能的**子集**.

!!! note
	**非机器学习**类人工智能, 就是传统的人工智能, 基本上都是有着严格规则一种算法. 例如各类游戏 AI (Alpha-Beta 剪枝, 寻路算法).

机器学习与非机器学习 AI 的区别

| 特征   | 机器学习        | 非机器学习      |
| ---- | ----------- | ---------- |
| 先验数据 | 必须          | 非必须        |
| 数学模型 | 推理模型+训练算法   | 推理模型       |
| 模型参数 | 机器自动习得      | 人工设计       |
| 准确率  | 理论上达不到 100% | 很容易达到 100% |
| 推理   | 计算机         | 计算机        |

!!! note
	这里强调的"推理模型+训练算法"实际上是强调了机器学习类可以自行更新和加强参数, 通过推理模型根据输入的 $x$ 输出 $y'$, 而训练算法则根据 $y$ 和 $y'$ 的关系不断更新推理模型的参数. 非机器学习类在人工写好模型后就不会再发生改变.

- 练习: 列出的几种算法中哪一种属于机器学习?

(1), 剪枝算法; (2), 深度优先算法; (3), 蒙特卡洛树搜索; (4), 启发式搜索; (5), 蚁群算法; (6), 宽度优先算法; (7), 最大-最小算法; (8), 线性回归

!!! success "参考答案"
	(8). 线性回归的推理模型是 $y=bx+a$, 使用最小二乘法等最小化误差的方式作为训练算法. 对于其他几项, 和机器学习类 AI 比较接近的都是启发式算法. 要么就是传统的图算法. 
	
	首先对于剪枝算法, 深度优先算法, 宽度优先算法, 最大最小搜索, 这几个都是图或者树的算法, 我们不需要进行训练, 一旦写死了代码其运行方式就不会改变.
	
	其次对于蚁群算法, 蒙特卡洛树算法和最大最小搜索算法, 虽然有了随机化模拟, 但是不会更新模型的内部参数, 因此也不属于机器学习的范畴.

- 机器学习的五个要素: 数据, 模型, 训练, 预测, 评估

实际上就是我们获得机器学习类模型的几个动作.

首先应当获取可靠的数据集, 接下来确定我们使用的模型, 而后使用数据集训练模型调整参数, 将训练后的模型用于新数据的预测, 最后由我们根据预测的准确性和其他性能评估模型.

### 2. 2 过拟合
!!! question
	训练集的结果是不是越准确越好?

!!! question
	为什么我们必须要分出训练集和测试集?

答: 不是越准确越好. 我们以最简单的线性回归模型举例, 假设存在一组数据与对应的映射

$$
X:\{1,2,3,4,5,6,7,8,9\}\longrightarrow Y:\{2,4,6,8,7,12,14,16,18\}
$$

我们很容易观察到, $Y$ 与 $X$ 的映射关系应当是 $Y=2X$, 但是当我们训练过度之后, 在 $X=5$ 的时候模型很可能输出一个接近于 $7$ 的数值. 我们称这种**在训练集上表现很好, 预测时表现很差**的现象称之为**过拟合**. 

解决过拟合问题的最佳方式就是**对收集到的数据进行预处理**, 类似于我们处理实验数据的方式, 将明显和总体趋势相悖的数据除掉, 确保训练数据的质量.

!!! quote
	**没有免费午餐定理**(NFL): 任何机器学习模型在所有问题上都是最好的. 换言之, **一个模型在某些任务上很厉害, 在另外的一些任务上一定会表现很差**. 我们只能尽可能选择合适的数据集和模型进行处理.
	
	例如, 按照上面我们解决过拟合问题的思路, 除掉 $X=5$ 对应的一组数据, 或许可以提高整体的泛化能力, 但是在这测试集组数据上的效果就没有过拟合的好.

### 2. 3 机器学习的分类

| 学习类别    | 是否存在标签 | 是否存在反馈             | 学习方式                   | 实例           |
| ------- | ------ | ------------------ | ---------------------- | ------------ |
| 监督学习    | 是      | 是(直接给出正确答案)        | 拟合输入输出关系               | 回归与分类        |
| 无监督学习   | 否      | 否                  | 机器自行寻找数据集潜在关系          | 聚类, 异常检测, 降维 |
| *半监督学习* | 是(少部分) | 是                  | 混合模式                   | -            |
| 强化学习    | 否      | 是(不知道答案, 但是存在奖励机制) | 不断试错, 不断强化思想钢印, 某种答案最优 | 决策类          |

(ps: 半监督学习了解即可)


## Chap III 无监督学习: 回归与分类模型
---
### 3. 1 常用损失函数
#### 3. 1. 1 回归损失函数
均方误差(mean squared error). 实际上就是方差.

$$
MSE=\frac{1}{N}\sum_{i=1}^N(Y_i-Y'_i)^2
$$

平均绝对误差(mean absolute error).

$$
MAE=\frac{1}{N}\sum_{i=1}^{N}\left|Y_i-Y'_i\right|
$$

平均绝对百分比误差(mean absolute percentage error).

$$
MAPE=\frac{1}{N}\sum_{i=1}^{N}\left|\frac{Y_i-Y'_i}{Y_i}\right|
$$

均方根对数误差(mean squared log error).

$$
MSLE=\frac{1}{N}\sum_{i=1}^{N}(\log \frac{Y'_i+1}{Y_i+1})^2
$$

#### 3. 1. 2 分类损失函数
!!! note
	个人认为没必要记. 太难记住了.

#### 3. 1. 3 一般终止条件
- 损失函数值足够小
- 损失函数值的变化足够小
- 训练次数足够多
- 梯度(就是模型参数变化)趋于 0
- 预测集准确率下降

### 3. 2 聚类
属于**无监督学习**, 根据数据点之间特征的**相似度**或距离(欧氏距离)将相似的数据点聚集在一起形成簇.

!!! note
	**欧氏距离**, 线性代数中的概念. 在学习线性代数时我们知道, 欧式空间是线性空间中的一种, n 维空间中的欧氏距离公式是
	
	$$
	d(X,Y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}
	$$

下面我们以典型的 **K-means Algorithm** (**K均值算法**) 为例说明聚类的过程. 

首先明确最终目的, 将数据集 $S$ 按照特征分类为 $n$ 组. 我们设 $\forall x\in S$ 都对应了 $n$ 个特征, $S$ 的大小(不同 $x$ 的个数)为 $N$. 我们将每个 $x$ 的 $n$ 个特征映射到一个 $n$ 维空间中, 于是我们得到了一个存在 $N$ 个点的空间.  

现在我们假设要将之分为 $m$ 组, 那么我们首先随机初始化 $m$ 个聚类质心, 然后不断循环下面两步:

- 计算每个数据到质心的距离, 按照如欧拉距离归类到距离最近的质心, 对应相同质心的点集我们称为簇.
- 调整质心位置, 具体做法是将当前簇内每个点距离之和的(某种)平均值最近的一处作为质心的位置.

直到:

- 达到迭代次数.
- 前后两次迭代中, 聚类的质心位置基本不变, 趋于稳定.

![](https://HollowDobt.github.io/picx-images-hosting/output-(7).6bhbzlxxng.webp)

参考演示网站: <http://alekseynp.com/viz/k-means.html>

### 3. 3 降维: 主成分分析(PCA)
目标: 在降维得到的新坐标系中实现数据波动(例如方差)的最大化.

视觉上就是建立一个新的坐标系, 将多个维度的数据投影到这个新的坐标系, 实现数据的降维. **新的坐标系轴方向就是主成分**.

在代数上, 就是将多个数据分组组合成一个新的特征, 提取最大波动的几个特征作为主特征.

![](https://HollowDobt.github.io/picx-images-hosting/output-(6).99tm343gpz.webp)


## Chap IV 深度学习(Deep Learning)
---

### 4. 1 多层人工神经网络与深度学习

!!! note
	深度学习**既不指监督学习或无监督学习, 也不指强化学习**. 其本身是前面几种学习类型, 也就是机器学习的实现方式.

深度学习(Deep Learning, 简称 DL)是机器学习的一个子集, 它使用**多层人工神经网络**来精准完成图像检测等任务. 通过**多层表示+高阶特征提取**完成如图像识别, 语音识别等各种任务, 具有较强的**特征鲁棒性(抗干扰)**.

![](https://HollowDobt.github.io/picx-images-hosting/output-(8).2h8kgtg188.webp)

深度学习的几个特征:

- 多层网络结构

分为**输入层, 隐藏层, 输出层**. 各层之间通过权重进行连接, 构成复杂的网络结构. 一般而言, 输入层用于"侦测和传递数据", 输出层用于将抽象信息具体化便于我们理解, 隐藏层负责提取特征进行分析.

- 自动特征提取

自动从原始数据中提取特征, 无需人为提取特征.

!!! note
	传统的机器学习需要进行人工特征提取. 比如一张图, 首先要人为提取出图片的整体纹理, 颜色直方图, 边缘提取等等, 既耗时又耗力, 而且难以泛化.
	
	深度学习通过多个隐藏层从低级特征到特征组合进行学习, 全流程自动化, 不需要人工干预.

- 非线性激活函数

深度学习模型通常使用非线性激活函数(比如 ReLU, sigmoid 等等)结合多层设计可以轻松处理复杂的非线性关系.

!!! note
	如果使用线性激活函数, 那么对于每个映射 $y=f(x)$, 都可以写作 $y=k_ix+b_i$. 假设是一个三层神经网络, 则激活函数等价于 $y=k_3(k_2(k_1x+b_1)+b_2)+b_3$, 打开合并后变成 $y=Kx+B$. 也就是说, 如果使用线性激活函数, 再多的层最后都等价于一个层.

- 大规模数据处理能力

综合前面几个优点, 尤其是自动特征提取, 使用多层人工神经网络的深度学习可以实现这一点.

### 4. 2 感知机

#### 4. 2. 1 感知机模型

这是经典的人工神经网络模型. 简单来讲, 感知机模拟了一个神经元的行为. 例如, $f$ 是一个非线性函数(就是激活函数), 而 $X$ 是一个向量矩阵. 这一模型提供一个线性函数, 将 $X$ 映射为 $\Sigma$, 其规则表述为 $\Sigma=WX^T+B$. 之后 $Y$ 作为非线性函数检测 $\Sigma$ 是否可以激活.

$$
Y=f(\Sigma),\
$$

在考试中, 会要求我们根据图写出 $Y$ 或者 $f$ (激活函数)的公式. 对于上述的映射, 我们可以形式化地表述公式为

$$
Y=f(WX^T+B)
$$

当然, 不适用矩阵也可以表达

$$
Y=f(\sum_{i=1}^{n}(w_ix_i+b))
$$

#### 4. 2. 2 多层感知机(MLP)

多层感知机(Multi-Layer Perceptron, 简称 MLP)的目的是解决非线性问题. 一个多层感知机至少包括三层: 输入层, 隐藏层, 输出层.

我们称只含有一个隐藏层的神经网络叫做**浅层学习网络**, 而将大于一个隐藏层的成为**深度学习网络**.

#### 4. 2. 3 常见激活函数
- ReLU 函数(线性整流函数)

计算式定义

$$
f(x)=\max(0,x)
$$


## 幕间: 关系总结
---

人工智能分为两大类, 一类是机器学习, 另一类是非机器学习. 前者除了模型本身, 还有训练算法, 可以不断增强自己的参数. 后者基本属于传统算法范畴, 在本门课程基本不做讨论.

对于机器学习, 我们可以从模型设计和训练算法两方面来分类. 

对于训练算法, 我们分为监督学习, 无监督学习和强化学习, 回归是典型的监督学习, 聚类和降维则是典型的无监督学习. 

对于模型设计, 我们主要讨论的是深度学习. 深度学习利用深层神经网络, 实现层级的特征抽象提取和表示学习. 现在最有名的基于 Transformer 架构的各类大模型, 就是典型的深度学习模型.

如果你是一位资深的 AI 用户, 你会发现几乎所有的 AI 都可以做到和你对话. 这本身就是件很了不起的事. 这一点归功于**预训练模型**. 预训练模型在深度学习模型的基础上, 使用大数据得到基本通用知识, 再迁移到各种具体任务中使用专业数据集进一步训练.

而当预训练模型被调教好后, 我们就得到了**生成式大模型**. 生成式大模型就是根据上下文自动生成相对高质量内容的**大规模人工神经网络**. 这其中我们最为熟悉, 最常用的便是**生成式大语言模型**.