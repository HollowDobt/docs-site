<!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Flowers' Fade, Leaves' Birth."><meta name=author content="Hollow Dobt"><link href=https://docs.hollowlib.top/ZJU-Courses-Notes/A/AIBasic%28A%29/ rel=canonical><link href=../NorCheT%28B%29/ rel=prev><link href=../Electron/ rel=next><link rel=icon href=../../../_assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.23"><title>人工智能基础 A - Hollow's Library</title><link rel=stylesheet href=../../../assets/stylesheets/main.84d31ad4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><style>:root{.md-tag.md-tag--learn{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M384%20512H96c-53%200-96-43-96-96V96C0%2043%2043%200%2096%200h304c26.5%200%2048%2021.5%2048%2048v288c0%2020.9-13.4%2038.7-32%2045.3V448c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032zM96%20384c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h256v-64zm32-232c0%2013.3%2010.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24H152c-13.3%200-24%2010.7-24%2024m24%2072c-13.3%200-24%2010.7-24%2024s10.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24z%22/%3E%3C/svg%3E');}}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../_stylesheets/extra.css><link rel=stylesheet href=../../../static/dist/output.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-TCDSJEFYY3"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-TCDSJEFYY3",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-TCDSJEFYY3",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=custom data-md-color-accent=custom> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#a class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=https://docs.hollowlib.top title="Hollow's Library" class="md-header__button md-logo" aria-label="Hollow's Library" data-md-component=logo> <img src=../../../_assets/favicon.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Hollow's Library </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 人工智能基础 A </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/hollowdobt/docs-site title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"></path></svg> </div> <div class=md-source__repository> hollowdobt/docs-site </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M240 6.1c9.1-8.2 22.9-8.2 32 0l232 208c9.9 8.8 10.7 24 1.8 33.9s-24 10.7-33.9 1.8l-8-7.2v205.3c0 35.3-28.7 64-64 64h-288c-35.3 0-64-28.7-64-64V242.6l-8 7.2c-9.9 8.8-25 8-33.9-1.8s-8-25 1.8-33.9zm16 50.1L96 199.7V448c0 8.8 7.2 16 16 16h48V360c0-39.8 32.2-72 72-72h48c39.8 0 72 32.2 72 72v104h48c8.8 0 16-7.2 16-16V199.7L256 56.3zM208 464h96V360c0-13.3-10.7-24-24-24h-48c-13.3 0-24 10.7-24 24z"></path></svg> Hollow's Library </a> </li> <li class=md-tabs__item> <a href=../../../programlang/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 384 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M64 48h112v88c0 39.8 32.2 72 72 72h88v240c0 8.8-7.2 16-16 16H64c-8.8 0-16-7.2-16-16V64c0-8.8 7.2-16 16-16m160 19.9 92.1 92.1H248c-13.3 0-24-10.7-24-24zM64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V186.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0zm106.2 295.6c8.6-10.1 7.5-25.2-2.6-33.8s-25.2-7.5-33.8 2.6l-48 56c-7.7 9-7.7 22.2 0 31.2l48 56c8.6 10.1 23.8 11.2 33.8 2.6s11.2-23.8 2.6-33.8L135.6 336zm80-31.2c-8.6-10.1-23.8-11.2-33.8-2.6s-11.2 23.8-2.6 33.8l34.6 40.4-34.6 40.4c-8.6 10.1-7.5 25.2 2.6 33.8s25.2 7.5 33.8-2.6l48-56c7.7-9 7.7-22.2 0-31.2z"></path></svg> 程序设计语言 </a> </li> <li class=md-tabs__item> <a href=../../../ai/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 256c0-114.9 93.1-208 208-208 63.1 0 119.6 28.1 157.8 72.5 8.6 10.1 23.8 11.2 33.8 2.6s11.2-23.8 2.6-33.8C403.3 34.6 333.7 0 256 0 114.6 0 0 114.6 0 256v40c0 13.3 10.7 24 24 24s24-10.7 24-24zm458.5-52.9c-2.7-13-15.5-21.3-28.4-18.5s-21.3 15.5-18.5 28.4c2.9 13.9 4.5 28.3 4.5 43.1v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40c0-18.1-1.9-35.8-5.5-52.9zM256 80c-19 0-37.4 3-54.5 8.6-15.2 5-18.7 23.7-8.3 35.9 7.1 8.3 18.8 10.8 29.4 7.9s21.8-4.4 33.4-4.4c70.7 0 128 57.3 128 128v24.9c0 25.2-1.5 50.3-4.4 75.3-1.7 14.6 9.4 27.8 24.2 27.8 11.8 0 21.9-8.6 23.3-20.3 3.3-27.4 5-55 5-82.7v-24.9c0-97.2-78.8-176-176-176zm-105.3 68.7c-9.1-10.6-25.3-11.4-33.9-.4C93.7 178.1 80 215.4 80 256v24.9c0 24.2-2.6 48.4-7.8 71.9-3.4 15.6 7.9 31.1 23.9 31.1 10.5 0 19.9-7 22.2-17.3 6.4-28.1 9.7-56.8 9.7-85.8v-24.9c0-27.2 8.5-52.4 22.9-73.1 7.2-10.4 8-24.6-.2-34.2zM256 160c-53 0-96 43-96 96v24.9c0 35.9-4.6 71.5-13.8 106.1-3.8 14.3 6.7 29 21.5 29 9.5 0 17.9-6.2 20.4-15.4 10.5-39 15.9-79.2 15.9-119.7V256c0-28.7 23.3-52 52-52s52 23.3 52 52v24.9c0 36.3-3.5 72.4-10.4 107.9-2.7 13.9 7.7 27.2 21.8 27.2 10.2 0 19-7 21-17 7.7-38.8 11.6-78.3 11.6-118.1V256c0-53-43-96-96-96m24 96c0-13.3-10.7-24-24-24s-24 10.7-24 24v24.9c0 59.9-11 119.3-32.5 175.2l-5.9 15.3c-4.8 12.4 1.4 26.3 13.8 31s26.3-1.4 31-13.8l5.9-15.3A536.2 536.2 0 0 0 280 280.9z"></path></svg> 人工智能 </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M96 512h320c17.7 0 32-14.3 32-32s-14.3-32-32-32v-66.7c18.6-6.6 32-24.4 32-45.3V48c0-26.5-21.5-48-48-48h-48v169.4c0 12.5-10.1 22.6-22.6 22.6-6 0-11.8-2.4-16-6.6L272 144l-41.4 41.4c-4.2 4.2-10 6.6-16 6.6-12.5 0-22.6-10.1-22.6-22.6V0H96C43 0 0 43 0 96v320c0 53 43 96 96 96m-32-96c0-17.7 14.3-32 32-32h256v64H96c-17.7 0-32-14.3-32-32"></path></svg> ZJU Courses Notes </a> </li> <li class=md-tabs__item> <a href=../../../algorithm/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M168 56v40H89.9C75.6 96 64 107.6 64 121.9c0 4 .9 8 2.7 11.6l33.4 66.8C88.7 202.1 79.9 212 79.9 224c0 13.3 10.7 24 24 24h5.6L95.9 384l-56.2 70.3c-5 6.3-7.8 14.1-7.8 22.2 0 19.6 15.9 35.5 35.5 35.5h248.9c19.6 0 35.5-15.9 35.5-35.5 0-8.1-2.7-15.9-7.8-22.2L288 384l-13.6-136h5.6c13.3 0 24-10.7 24-24 0-11.9-8.7-21.9-20.2-23.7l33.4-66.8c1.8-3.6 2.7-7.6 2.7-11.6 0-14.3-11.6-25.9-25.9-25.9h-78.1V56h16c13.3 0 24-10.7 24-24S245.3 8 232 8h-16V-8c0-13.3-10.7-24-24-24s-24 10.7-24 24V8h-16c-13.3 0-24 10.7-24 24s10.7 24 24 24zm157.8 223.6 8.5 85.5 47.4 59.2 4.2 5.7c9.2 13.7 14.1 29.9 14.1 46.5 0 12.7-2.8 24.8-7.9 35.5h83.1c20.3 0 36.8-16.5 36.8-36.8 0-7.3-2.2-14.4-6.2-20.4L480 416.1v-64l13.3-13.3c12-12 18.7-28.3 18.7-45.3V192c0-17.7-14.3-32-32-32s-32 14.3-32 32v16h-32v-16c0-17.7-14.3-32-32-32s-32 14.3-32 32v32c0 22.4-10.2 42.4-26.2 55.6"></path></svg> 算法分析 </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://docs.hollowlib.top title="Hollow's Library" class="md-nav__button md-logo" aria-label="Hollow's Library" data-md-component=logo> <img src=../../../_assets/favicon.png alt=logo> </a> Hollow's Library </label> <div class=md-nav__source> <a href=https://github.com/hollowdobt/docs-site title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"></path></svg> </div> <div class=md-source__repository> hollowdobt/docs-site </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../.. class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M240 6.1c9.1-8.2 22.9-8.2 32 0l232 208c9.9 8.8 10.7 24 1.8 33.9s-24 10.7-33.9 1.8l-8-7.2v205.3c0 35.3-28.7 64-64 64h-288c-35.3 0-64-28.7-64-64V242.6l-8 7.2c-9.9 8.8-25 8-33.9-1.8s-8-25 1.8-33.9zm16 50.1L96 199.7V448c0 8.8 7.2 16 16 16h48V360c0-39.8 32.2-72 72-72h48c39.8 0 72 32.2 72 72v104h48c8.8 0 16-7.2 16-16V199.7L256 56.3zM208 464h96V360c0-13.3-10.7-24-24-24h-48c-13.3 0-24 10.7-24 24z"></path></svg> <span class=md-ellipsis> Hollow's Library </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Hollow's Library </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=../../../programlang/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 384 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M64 48h112v88c0 39.8 32.2 72 72 72h88v240c0 8.8-7.2 16-16 16H64c-8.8 0-16-7.2-16-16V64c0-8.8 7.2-16 16-16m160 19.9 92.1 92.1H248c-13.3 0-24-10.7-24-24zM64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V186.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0zm106.2 295.6c8.6-10.1 7.5-25.2-2.6-33.8s-25.2-7.5-33.8 2.6l-48 56c-7.7 9-7.7 22.2 0 31.2l48 56c8.6 10.1 23.8 11.2 33.8 2.6s11.2-23.8 2.6-33.8L135.6 336zm80-31.2c-8.6-10.1-23.8-11.2-33.8-2.6s-11.2 23.8-2.6 33.8l34.6 40.4-34.6 40.4c-8.6 10.1-7.5 25.2 2.6 33.8s25.2 7.5 33.8-2.6l48-56c7.7-9 7.7-22.2 0-31.2z"></path></svg> <span class=md-ellipsis> 程序设计语言 </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> 程序设计语言 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> § C++ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> § C++ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../programlang/C%2B%2B/C%2B%2BOOP/ class=md-nav__link> <span class=md-ellipsis> C++ 面向对象程序设计 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../ai/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 256c0-114.9 93.1-208 208-208 63.1 0 119.6 28.1 157.8 72.5 8.6 10.1 23.8 11.2 33.8 2.6s11.2-23.8 2.6-33.8C403.3 34.6 333.7 0 256 0 114.6 0 0 114.6 0 256v40c0 13.3 10.7 24 24 24s24-10.7 24-24zm458.5-52.9c-2.7-13-15.5-21.3-28.4-18.5s-21.3 15.5-18.5 28.4c2.9 13.9 4.5 28.3 4.5 43.1v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40c0-18.1-1.9-35.8-5.5-52.9zM256 80c-19 0-37.4 3-54.5 8.6-15.2 5-18.7 23.7-8.3 35.9 7.1 8.3 18.8 10.8 29.4 7.9s21.8-4.4 33.4-4.4c70.7 0 128 57.3 128 128v24.9c0 25.2-1.5 50.3-4.4 75.3-1.7 14.6 9.4 27.8 24.2 27.8 11.8 0 21.9-8.6 23.3-20.3 3.3-27.4 5-55 5-82.7v-24.9c0-97.2-78.8-176-176-176zm-105.3 68.7c-9.1-10.6-25.3-11.4-33.9-.4C93.7 178.1 80 215.4 80 256v24.9c0 24.2-2.6 48.4-7.8 71.9-3.4 15.6 7.9 31.1 23.9 31.1 10.5 0 19.9-7 22.2-17.3 6.4-28.1 9.7-56.8 9.7-85.8v-24.9c0-27.2 8.5-52.4 22.9-73.1 7.2-10.4 8-24.6-.2-34.2zM256 160c-53 0-96 43-96 96v24.9c0 35.9-4.6 71.5-13.8 106.1-3.8 14.3 6.7 29 21.5 29 9.5 0 17.9-6.2 20.4-15.4 10.5-39 15.9-79.2 15.9-119.7V256c0-28.7 23.3-52 52-52s52 23.3 52 52v24.9c0 36.3-3.5 72.4-10.4 107.9-2.7 13.9 7.7 27.2 21.8 27.2 10.2 0 19-7 21-17 7.7-38.8 11.6-78.3 11.6-118.1V256c0-53-43-96-96-96m24 96c0-13.3-10.7-24-24-24s-24 10.7-24 24v24.9c0 59.9-11 119.3-32.5 175.2l-5.9 15.3c-4.8 12.4 1.4 26.3 13.8 31s26.3-1.4 31-13.8l5.9-15.3A536.2 536.2 0 0 0 280 280.9z"></path></svg> <span class=md-ellipsis> 人工智能 </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> 人工智能 </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M96 512h320c17.7 0 32-14.3 32-32s-14.3-32-32-32v-66.7c18.6-6.6 32-24.4 32-45.3V48c0-26.5-21.5-48-48-48h-48v169.4c0 12.5-10.1 22.6-22.6 22.6-6 0-11.8-2.4-16-6.6L272 144l-41.4 41.4c-4.2 4.2-10 6.6-16 6.6-12.5 0-22.6-10.1-22.6-22.6V0H96C43 0 0 43 0 96v320c0 53 43 96 96 96m-32-96c0-17.7 14.3-32 32-32h256v64H96c-17.7 0-32-14.3-32-32"></path></svg> <span class=md-ellipsis> ZJU Courses Notes </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> ZJU Courses Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> § 微积分(甲)II </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> § 微积分(甲)II </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Calculus%28A%29II/ class=md-nav__link> <span class=md-ellipsis> 微积分(甲) II 复习笔记 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> § 普通化学实验(乙) </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> § 普通化学实验(乙) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../NorCheT%28B%29/ class=md-nav__link> <span class=md-ellipsis> 普通化学实验(乙) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_4 checked> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex=0> <span class=md-ellipsis> § 人工智能基础(A) </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> § 人工智能基础(A) </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 人工智能基础 A </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 人工智能基础 A </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-chap-i class=md-nav__link> <span class=md-ellipsis> 1. Chap I 人工智能起源 </span> </a> <nav class=md-nav aria-label="1. Chap I 人工智能起源"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-1-1 class=md-nav__link> <span class=md-ellipsis> 1.1 1. 1 人工智能的诞生 </span> </a> </li> <li class=md-nav__item> <a href=#12-1-2 class=md-nav__link> <span class=md-ellipsis> 1.2 1. 2 人工智能的三大学派 </span> </a> </li> <li class=md-nav__item> <a href=#13-1-3 class=md-nav__link> <span class=md-ellipsis> 1.3 1. 3 幸存者偏差 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-chap-ii class=md-nav__link> <span class=md-ellipsis> 2. Chap II 机器学习 </span> </a> <nav class=md-nav aria-label="2. Chap II 机器学习"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-2-1 class=md-nav__link> <span class=md-ellipsis> 2.1 2. 1 机器学习的定义 </span> </a> </li> <li class=md-nav__item> <a href=#22-2-2 class=md-nav__link> <span class=md-ellipsis> 2.2 2. 2 过拟合 </span> </a> </li> <li class=md-nav__item> <a href=#23-2-3 class=md-nav__link> <span class=md-ellipsis> 2.3 2. 3 机器学习的分类 </span> </a> </li> <li class=md-nav__item> <a href=#24-2-4 class=md-nav__link> <span class=md-ellipsis> 2.4 2. 4 线性回归与最小二乘法 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-chap-iii class=md-nav__link> <span class=md-ellipsis> 3. Chap III 无监督学习: 回归与分类模型 </span> </a> <nav class=md-nav aria-label="3. Chap III 无监督学习: 回归与分类模型"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-3-1 class=md-nav__link> <span class=md-ellipsis> 3.1 3. 1 常用损失函数 </span> </a> <nav class=md-nav aria-label="3.1 3. 1 常用损失函数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#311-3-1-1 class=md-nav__link> <span class=md-ellipsis> 3.1.1 3. 1. 1 回归损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#312-3-1-2 class=md-nav__link> <span class=md-ellipsis> 3.1.2 3. 1. 2 分类损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#313-3-1-3 class=md-nav__link> <span class=md-ellipsis> 3.1.3 3. 1. 3 一般终止条件 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32-3-2 class=md-nav__link> <span class=md-ellipsis> 3.2 3. 2 聚类 </span> </a> </li> <li class=md-nav__item> <a href=#33-3-3-pca class=md-nav__link> <span class=md-ellipsis> 3.3 3. 3 降维: 主成分分析(PCA) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-chap-iv-deep-learning class=md-nav__link> <span class=md-ellipsis> 4. Chap IV 深度学习(Deep Learning)导论 </span> </a> <nav class=md-nav aria-label="4. Chap IV 深度学习(Deep Learning)导论"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41-4-1 class=md-nav__link> <span class=md-ellipsis> 4.1 4. 1 多层人工神经网络与深度学习 </span> </a> </li> <li class=md-nav__item> <a href=#42-4-2 class=md-nav__link> <span class=md-ellipsis> 4.2 4. 2 感知机 </span> </a> <nav class=md-nav aria-label="4.2 4. 2 感知机"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#421-4-2-1 class=md-nav__link> <span class=md-ellipsis> 4.2.1 4. 2. 1 感知机模型 </span> </a> </li> <li class=md-nav__item> <a href=#422-4-2-2-mlp class=md-nav__link> <span class=md-ellipsis> 4.2.2 4. 2. 2 多层感知机(MLP) </span> </a> </li> <li class=md-nav__item> <a href=#423-4-2-3 class=md-nav__link> <span class=md-ellipsis> 4.2.3 4. 2. 3 常见激活函数 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#43-4-3-bp class=md-nav__link> <span class=md-ellipsis> 4.3 4. 3 反向传播算法: BP </span> </a> </li> <li class=md-nav__item> <a href=#44-4-4-gd class=md-nav__link> <span class=md-ellipsis> 4.4 4. 4 梯度下降: GD </span> </a> </li> <li class=md-nav__item> <a href=#45-4-5 class=md-nav__link> <span class=md-ellipsis> 4.5 4. 5 梯度下降算法的改进: 优化器 </span> </a> <nav class=md-nav aria-label="4.5 4. 5 梯度下降算法的改进: 优化器"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#451-4-5-1-sgd class=md-nav__link> <span class=md-ellipsis> 4.5.1 4. 5. 1 随机梯度下降法(SGD) </span> </a> </li> <li class=md-nav__item> <a href=#452-4-5-2-adagrad class=md-nav__link> <span class=md-ellipsis> 4.5.2 4. 5. 2 自适应梯度算法(AdaGrad) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#5 class=md-nav__link> <span class=md-ellipsis> 5. 幕间: 关系总结 </span> </a> </li> <li class=md-nav__item> <a href=#6-chap-v class=md-nav__link> <span class=md-ellipsis> 6. Chap V 深度学习实例 </span> </a> <nav class=md-nav aria-label="6. Chap V 深度学习实例"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#61-5-1-cnn class=md-nav__link> <span class=md-ellipsis> 6.1 5. 1 CNN 卷积神经网络 </span> </a> <nav class=md-nav aria-label="6.1 5. 1 CNN 卷积神经网络"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#611-5-1-1 class=md-nav__link> <span class=md-ellipsis> 6.1.1 5. 1. 1 卷积运算 </span> </a> </li> <li class=md-nav__item> <a href=#612-5-1-2 class=md-nav__link> <span class=md-ellipsis> 6.1.2 5. 1. 2 池化 </span> </a> </li> <li class=md-nav__item> <a href=#613-5-1-3 class=md-nav__link> <span class=md-ellipsis> 6.1.3 5. 1. 3 本质 </span> </a> </li> <li class=md-nav__item> <a href=#614-5-1-4 class=md-nav__link> <span class=md-ellipsis> 6.1.4 5. 1. 4 计算 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#62 class=md-nav__link> <span class=md-ellipsis> 6.2 幕间: 三种神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#63-5-2-rnn class=md-nav__link> <span class=md-ellipsis> 6.3 5. 2 RNN 循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#64-5-3-lstm class=md-nav__link> <span class=md-ellipsis> 6.4 5. 3 LSTM 长短程记忆网络 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#7-chap-vi-nlp class=md-nav__link> <span class=md-ellipsis> 7. Chap VI 自然语言处理(NLP) </span> </a> <nav class=md-nav aria-label="7. Chap VI 自然语言处理(NLP)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#71-6-1 class=md-nav__link> <span class=md-ellipsis> 7.1 6. 1 历史 </span> </a> </li> <li class=md-nav__item> <a href=#72-6-2 class=md-nav__link> <span class=md-ellipsis> 7.2 6. 2 分词 </span> </a> </li> <li class=md-nav__item> <a href=#73-6-3 class=md-nav__link> <span class=md-ellipsis> 7.3 6. 3 文本相似度计算 </span> </a> </li> <li class=md-nav__item> <a href=#74-6-4-transformer class=md-nav__link> <span class=md-ellipsis> 7.4 6. 4 现代自然语言处理框架: Transformer </span> </a> <nav class=md-nav aria-label="7.4 6. 4 现代自然语言处理框架: Transformer"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#741-6-4-1 class=md-nav__link> <span class=md-ellipsis> 7.4.1 6. 4. 1 简述 </span> </a> </li> <li class=md-nav__item> <a href=#742-6-4-2 class=md-nav__link> <span class=md-ellipsis> 7.4.2 6. 4. 2 自注意力机制 </span> </a> </li> <li class=md-nav__item> <a href=#743-6-4-3 class=md-nav__link> <span class=md-ellipsis> 7.4.3 6. 4. 3 三种模型 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#8-chap-vii-aigc-llm class=md-nav__link> <span class=md-ellipsis> 8. Chap VII AIGC 与 LLM </span> </a> <nav class=md-nav aria-label="8. Chap VII AIGC 与 LLM"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#81-7-1 class=md-nav__link> <span class=md-ellipsis> 8.1 7. 1 绪论 </span> </a> </li> <li class=md-nav__item> <a href=#82-7-2 class=md-nav__link> <span class=md-ellipsis> 8.2 7. 2 名词解释 </span> </a> </li> <li class=md-nav__item> <a href=#83-7-3 class=md-nav__link> <span class=md-ellipsis> 8.3 7. 3 大语言模型的特征 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#9-i-python-python class=md-nav__link> <span class=md-ellipsis> 9. 附则 I python 基础与机器学习常用 python 库 </span> </a> </li> <li class=md-nav__item> <a href=#10-ii class=md-nav__link> <span class=md-ellipsis> 10. 附则 II 补充相关知识 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_5> <div class="md-nav__link md-nav__container"> <a href=../Electron/ class="md-nav__link "> <span class=md-ellipsis> § 电工电子学 </span> </a> <label class="md-nav__link " for=__nav_4_5 id=__nav_4_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> § 电工电子学 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Electron/Circuitsandelectroniccomponents/ class=md-nav__link> <span class=md-ellipsis> • Chap I 电路和电路元件 </span> </a> </li> <li class=md-nav__item> <a href=../Electron/CircuitAnalysisBasics/ class=md-nav__link> <span class=md-ellipsis> • Chap II 电路分析基础 </span> </a> </li> <li class=md-nav__item> <a href=../Electron/BaseDivi/ class=md-nav__link> <span class=md-ellipsis> • Chap III 分立元件基本电路 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_6> <div class="md-nav__link md-nav__container"> <a href=../TheoreticalMechanics/ class="md-nav__link "> <span class=md-ellipsis> § 理论力学(甲) </span> </a> <label class="md-nav__link " for=__nav_4_6 id=__nav_4_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_6> <span class="md-nav__icon md-icon"></span> § 理论力学(甲) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../TheoreticalMechanics/pointmove/ class=md-nav__link> <span class=md-ellipsis> • Chap I 点的运动学 </span> </a> </li> <li class=md-nav__item> <a href=../TheoreticalMechanics/rigidbodymove/ class=md-nav__link> <span class=md-ellipsis> • Chap II 刚体运动学 </span> </a> </li> <li class=md-nav__item> <a href=../TheoreticalMechanics/combinationmove/ class=md-nav__link> <span class=md-ellipsis> • Chap III 点的合成运动 </span> </a> </li> <li class=md-nav__item> <a href=../TheoreticalMechanics/rigidbodyinflat/ class=md-nav__link> <span class=md-ellipsis> • Chap IV 刚体的平面运动 </span> </a> </li> <li class=md-nav__item> <a href=../TheoreticalMechanics/forcetheory/ class=md-nav__link> <span class=md-ellipsis> • Chap V 静力学公理与受力分析 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_7> <div class="md-nav__link md-nav__container"> <a href=../PhysicsII/ class="md-nav__link "> <span class=md-ellipsis> § 大学物理(乙) II </span> </a> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_7_label aria-expanded=false> <label class=md-nav__title for=__nav_4_7> <span class="md-nav__icon md-icon"></span> § 大学物理(乙) II </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_8> <div class="md-nav__link md-nav__container"> <a href=../BigData/ class="md-nav__link "> <span class=md-ellipsis> § 人工智能: 大规模数据分析与机器学习模型中的算法优化 </span> </a> <label class="md-nav__link " for=__nav_4_8 id=__nav_4_8_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_8_label aria-expanded=false> <label class=md-nav__title for=__nav_4_8> <span class="md-nav__icon md-icon"></span> § 人工智能: 大规模数据分析与机器学习模型中的算法优化 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../BigData/GameTheory/ class=md-nav__link> <span class=md-ellipsis> • LEC 1 Game Theory and Lower Bounds for Randomized Algorithms </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../../../algorithm/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M168 56v40H89.9C75.6 96 64 107.6 64 121.9c0 4 .9 8 2.7 11.6l33.4 66.8C88.7 202.1 79.9 212 79.9 224c0 13.3 10.7 24 24 24h5.6L95.9 384l-56.2 70.3c-5 6.3-7.8 14.1-7.8 22.2 0 19.6 15.9 35.5 35.5 35.5h248.9c19.6 0 35.5-15.9 35.5-35.5 0-8.1-2.7-15.9-7.8-22.2L288 384l-13.6-136h5.6c13.3 0 24-10.7 24-24 0-11.9-8.7-21.9-20.2-23.7l33.4-66.8c1.8-3.6 2.7-7.6 2.7-11.6 0-14.3-11.6-25.9-25.9-25.9h-78.1V56h16c13.3 0 24-10.7 24-24S245.3 8 232 8h-16V-8c0-13.3-10.7-24-24-24s-24 10.7-24 24V8h-16c-13.3 0-24 10.7-24 24s10.7 24 24 24zm157.8 223.6 8.5 85.5 47.4 59.2 4.2 5.7c9.2 13.7 14.1 29.9 14.1 46.5 0 12.7-2.8 24.8-7.9 35.5h83.1c20.3 0 36.8-16.5 36.8-36.8 0-7.3-2.2-14.4-6.2-20.4L480 416.1v-64l13.3-13.3c12-12 18.7-28.3 18.7-45.3V192c0-17.7-14.3-32-32-32s-32 14.3-32 32v16h-32v-16c0-17.7-14.3-32-32-32s-32 14.3-32 32v32c0 22.4-10.2 42.4-26.2 55.6"></path></svg> <span class=md-ellipsis> 算法分析 </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> 算法分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <div class="md-nav__link md-nav__container"> <a href=../../../algorithm/DBAA/ class="md-nav__link "> <span class=md-ellipsis> § 数据结构基础与算法分析 </span> </a> <label class="md-nav__link " for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> § 数据结构基础与算法分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../algorithm/DBAA/array/ class=md-nav__link> <span class=md-ellipsis> • Chap I 线性表 </span> </a> </li> <li class=md-nav__item> <a href=../../../algorithm/DBAA/appendix1/ class=md-nav__link> <span class=md-ellipsis> • Appe I 数学基础与算法设计原则 </span> </a> </li> <li class=md-nav__item> <a href=../../../algorithm/DBAA/appendix2/ class=md-nav__link> <span class=md-ellipsis> • Appe II 算法分析语设计实例 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> § Codeforces </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> § Codeforces </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../algorithm/cf/stencil/ class=md-nav__link> <span class=md-ellipsis> 模版 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-chap-i class=md-nav__link> <span class=md-ellipsis> 1. Chap I 人工智能起源 </span> </a> <nav class=md-nav aria-label="1. Chap I 人工智能起源"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-1-1 class=md-nav__link> <span class=md-ellipsis> 1.1 1. 1 人工智能的诞生 </span> </a> </li> <li class=md-nav__item> <a href=#12-1-2 class=md-nav__link> <span class=md-ellipsis> 1.2 1. 2 人工智能的三大学派 </span> </a> </li> <li class=md-nav__item> <a href=#13-1-3 class=md-nav__link> <span class=md-ellipsis> 1.3 1. 3 幸存者偏差 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-chap-ii class=md-nav__link> <span class=md-ellipsis> 2. Chap II 机器学习 </span> </a> <nav class=md-nav aria-label="2. Chap II 机器学习"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-2-1 class=md-nav__link> <span class=md-ellipsis> 2.1 2. 1 机器学习的定义 </span> </a> </li> <li class=md-nav__item> <a href=#22-2-2 class=md-nav__link> <span class=md-ellipsis> 2.2 2. 2 过拟合 </span> </a> </li> <li class=md-nav__item> <a href=#23-2-3 class=md-nav__link> <span class=md-ellipsis> 2.3 2. 3 机器学习的分类 </span> </a> </li> <li class=md-nav__item> <a href=#24-2-4 class=md-nav__link> <span class=md-ellipsis> 2.4 2. 4 线性回归与最小二乘法 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-chap-iii class=md-nav__link> <span class=md-ellipsis> 3. Chap III 无监督学习: 回归与分类模型 </span> </a> <nav class=md-nav aria-label="3. Chap III 无监督学习: 回归与分类模型"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-3-1 class=md-nav__link> <span class=md-ellipsis> 3.1 3. 1 常用损失函数 </span> </a> <nav class=md-nav aria-label="3.1 3. 1 常用损失函数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#311-3-1-1 class=md-nav__link> <span class=md-ellipsis> 3.1.1 3. 1. 1 回归损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#312-3-1-2 class=md-nav__link> <span class=md-ellipsis> 3.1.2 3. 1. 2 分类损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#313-3-1-3 class=md-nav__link> <span class=md-ellipsis> 3.1.3 3. 1. 3 一般终止条件 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32-3-2 class=md-nav__link> <span class=md-ellipsis> 3.2 3. 2 聚类 </span> </a> </li> <li class=md-nav__item> <a href=#33-3-3-pca class=md-nav__link> <span class=md-ellipsis> 3.3 3. 3 降维: 主成分分析(PCA) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-chap-iv-deep-learning class=md-nav__link> <span class=md-ellipsis> 4. Chap IV 深度学习(Deep Learning)导论 </span> </a> <nav class=md-nav aria-label="4. Chap IV 深度学习(Deep Learning)导论"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41-4-1 class=md-nav__link> <span class=md-ellipsis> 4.1 4. 1 多层人工神经网络与深度学习 </span> </a> </li> <li class=md-nav__item> <a href=#42-4-2 class=md-nav__link> <span class=md-ellipsis> 4.2 4. 2 感知机 </span> </a> <nav class=md-nav aria-label="4.2 4. 2 感知机"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#421-4-2-1 class=md-nav__link> <span class=md-ellipsis> 4.2.1 4. 2. 1 感知机模型 </span> </a> </li> <li class=md-nav__item> <a href=#422-4-2-2-mlp class=md-nav__link> <span class=md-ellipsis> 4.2.2 4. 2. 2 多层感知机(MLP) </span> </a> </li> <li class=md-nav__item> <a href=#423-4-2-3 class=md-nav__link> <span class=md-ellipsis> 4.2.3 4. 2. 3 常见激活函数 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#43-4-3-bp class=md-nav__link> <span class=md-ellipsis> 4.3 4. 3 反向传播算法: BP </span> </a> </li> <li class=md-nav__item> <a href=#44-4-4-gd class=md-nav__link> <span class=md-ellipsis> 4.4 4. 4 梯度下降: GD </span> </a> </li> <li class=md-nav__item> <a href=#45-4-5 class=md-nav__link> <span class=md-ellipsis> 4.5 4. 5 梯度下降算法的改进: 优化器 </span> </a> <nav class=md-nav aria-label="4.5 4. 5 梯度下降算法的改进: 优化器"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#451-4-5-1-sgd class=md-nav__link> <span class=md-ellipsis> 4.5.1 4. 5. 1 随机梯度下降法(SGD) </span> </a> </li> <li class=md-nav__item> <a href=#452-4-5-2-adagrad class=md-nav__link> <span class=md-ellipsis> 4.5.2 4. 5. 2 自适应梯度算法(AdaGrad) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#5 class=md-nav__link> <span class=md-ellipsis> 5. 幕间: 关系总结 </span> </a> </li> <li class=md-nav__item> <a href=#6-chap-v class=md-nav__link> <span class=md-ellipsis> 6. Chap V 深度学习实例 </span> </a> <nav class=md-nav aria-label="6. Chap V 深度学习实例"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#61-5-1-cnn class=md-nav__link> <span class=md-ellipsis> 6.1 5. 1 CNN 卷积神经网络 </span> </a> <nav class=md-nav aria-label="6.1 5. 1 CNN 卷积神经网络"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#611-5-1-1 class=md-nav__link> <span class=md-ellipsis> 6.1.1 5. 1. 1 卷积运算 </span> </a> </li> <li class=md-nav__item> <a href=#612-5-1-2 class=md-nav__link> <span class=md-ellipsis> 6.1.2 5. 1. 2 池化 </span> </a> </li> <li class=md-nav__item> <a href=#613-5-1-3 class=md-nav__link> <span class=md-ellipsis> 6.1.3 5. 1. 3 本质 </span> </a> </li> <li class=md-nav__item> <a href=#614-5-1-4 class=md-nav__link> <span class=md-ellipsis> 6.1.4 5. 1. 4 计算 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#62 class=md-nav__link> <span class=md-ellipsis> 6.2 幕间: 三种神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#63-5-2-rnn class=md-nav__link> <span class=md-ellipsis> 6.3 5. 2 RNN 循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#64-5-3-lstm class=md-nav__link> <span class=md-ellipsis> 6.4 5. 3 LSTM 长短程记忆网络 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#7-chap-vi-nlp class=md-nav__link> <span class=md-ellipsis> 7. Chap VI 自然语言处理(NLP) </span> </a> <nav class=md-nav aria-label="7. Chap VI 自然语言处理(NLP)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#71-6-1 class=md-nav__link> <span class=md-ellipsis> 7.1 6. 1 历史 </span> </a> </li> <li class=md-nav__item> <a href=#72-6-2 class=md-nav__link> <span class=md-ellipsis> 7.2 6. 2 分词 </span> </a> </li> <li class=md-nav__item> <a href=#73-6-3 class=md-nav__link> <span class=md-ellipsis> 7.3 6. 3 文本相似度计算 </span> </a> </li> <li class=md-nav__item> <a href=#74-6-4-transformer class=md-nav__link> <span class=md-ellipsis> 7.4 6. 4 现代自然语言处理框架: Transformer </span> </a> <nav class=md-nav aria-label="7.4 6. 4 现代自然语言处理框架: Transformer"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#741-6-4-1 class=md-nav__link> <span class=md-ellipsis> 7.4.1 6. 4. 1 简述 </span> </a> </li> <li class=md-nav__item> <a href=#742-6-4-2 class=md-nav__link> <span class=md-ellipsis> 7.4.2 6. 4. 2 自注意力机制 </span> </a> </li> <li class=md-nav__item> <a href=#743-6-4-3 class=md-nav__link> <span class=md-ellipsis> 7.4.3 6. 4. 3 三种模型 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#8-chap-vii-aigc-llm class=md-nav__link> <span class=md-ellipsis> 8. Chap VII AIGC 与 LLM </span> </a> <nav class=md-nav aria-label="8. Chap VII AIGC 与 LLM"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#81-7-1 class=md-nav__link> <span class=md-ellipsis> 8.1 7. 1 绪论 </span> </a> </li> <li class=md-nav__item> <a href=#82-7-2 class=md-nav__link> <span class=md-ellipsis> 8.2 7. 2 名词解释 </span> </a> </li> <li class=md-nav__item> <a href=#83-7-3 class=md-nav__link> <span class=md-ellipsis> 8.3 7. 3 大语言模型的特征 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#9-i-python-python class=md-nav__link> <span class=md-ellipsis> 9. 附则 I python 基础与机器学习常用 python 库 </span> </a> </li> <li class=md-nav__item> <a href=#10-ii class=md-nav__link> <span class=md-ellipsis> 10. 附则 II 补充相关知识 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/hollowdobt/docs-site/edit/main/docs/ZJU-Courses-Notes/A/AIBasic(A).md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <a href=https://github.com/hollowdobt/docs-site/raw/main/docs/ZJU-Courses-Notes/A/AIBasic(A).md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg> </a> <h1 id=a>人工智能基础 A<a class=headerlink href=#a title="Permanent link">#</a></h1> <div class="admonition abstract"> <p class=admonition-title>Abstract</p> <p>想认认真真学人工智能千万不能选这门课. 想要水学分的也千万别选这门课. 哦, 是必修啊, 那没事儿了.</p> <p>这篇文档除了书本上的相关知识, 还有大量的补充和实践代码, 感兴趣的同学可以仔细阅读. 比较详细的代码我索引到了专门的人工智能章节. 对于期末考试, 个人认为主要看看历年卷记忆相关公式即可. 如果你想考比较高的分数, 你需要掌握这里面大部分框架的原理, 实现等等.</p> </div> <h2 id=1-chap-i>1. Chap I 人工智能起源<a class=headerlink href=#1-chap-i title="Permanent link">#</a></h2> <hr> <div class="admonition quote"> <p class=admonition-title>Quote</p> <p>人工智能的诞生早于现代程序设计语言. 当时召开达特茅斯会议时大家发现, 传统的数值计算类语言(比如 <code>FORTRAN</code> 语言)难以满足表示和操纵符号结构的需求, 据此诞生了诸如 <code>LISP</code> 等符号处理类语言. 这实际上也成为后来函数式编程和元编程的设计范式.</p> </div> <h3 id=11-1-1>1.1 1. 1 人工智能的诞生<a class=headerlink href=#11-1-1 title="Permanent link">#</a></h3> <p>1956 年, 达特茅斯会议的召开标志着人工智能元年的到来.</p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p><strong>图灵测试</strong>: 如果人无法在对话中区别与自己对话的是人还是机器, 那么这台机器就可以称为"智能".</p> </div> <h3 id=12-1-2>1.2 1. 2 人工智能的三大学派<a class=headerlink href=#12-1-2 title="Permanent link">#</a></h3> <ul> <li><strong>符号主义</strong></li> </ul> <p>人的本质是台机器, 强调逻辑与规则推理, 信息使用显示符号结构表示(比如 <code>LISP</code> 语言), 常作为逻辑推理系统.</p> <ul> <li><strong>行为主义</strong></li> </ul> <p>心理过程无法被观察, 不关注逻辑的"黑箱"过程, 通过强化机制(强化学习)学习.</p> <ul> <li><strong>联结主义</strong></li> </ul> <p>认知过程本质上是神经元之间的联结模式, 模拟人的神经网络结构构造人工神经网络.</p> <h3 id=13-1-3>1.3 1. 3 幸存者偏差<a class=headerlink href=#13-1-3 title="Permanent link">#</a></h3> <p>AI 的局限绝大多数起源于幸存者偏差现象. 我们在训练的过程中无意识或者有意识地忽略了很多信息, 导致特例成为了普遍现象.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>最典型的例子莫过于二战时期的飞机弹痕. 我们常常只关注成功的例子, 因为失败的例子我们难以直接观察到.</p> </div> <h2 id=2-chap-ii>2. Chap II 机器学习<a class=headerlink href=#2-chap-ii title="Permanent link">#</a></h2> <hr> <h3 id=21-2-1>2.1 2. 1 机器学习的定义<a class=headerlink href=#21-2-1 title="Permanent link">#</a></h3> <p>已知一个数据集 <span class=arithmatex>\(S\)</span>, 对任意输入的 <span class=arithmatex>\(x\in S\)</span> 都存在对应的标签 <span class=arithmatex>\(y\)</span>, 通过计算机系统寻找一个数学模型 <span class=arithmatex>\(f\)</span>, 使得对于任意 <span class=arithmatex>\(x\)</span>, 计算得到的<span class=arithmatex>\(y'=f(x)\)</span> 都尽可能的逼近于对应的 <span class=arithmatex>\(y\)</span>.</p> <ul> <li>每<strong>一对</strong> <span class=arithmatex>\((x,y)\)</span> 称为一个样本, 总个数叫做<strong>样本大小</strong>.</li> <li><span class=arithmatex>\(y\)</span> 的取值范围称为<strong>样本空间</strong>.</li> <li>集合 <span class=arithmatex>\(S\)</span> 是样本空间中的一个随机抽样.</li> <li>机器学习是人工智能的<strong>子集</strong>.</li> </ul> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>非机器学习</strong>类人工智能, 是传统的人工智能, 本质上都是有着严格规则一种算法. 例如各类游戏 AI (Alpha-Beta 剪枝, 寻路算法).</p> </div> <p>机器学习与非机器学习 AI 的区别</p> <table> <thead> <tr> <th>特征</th> <th>机器学习</th> <th>非机器学习</th> </tr> </thead> <tbody> <tr> <td>先验数据</td> <td>必须</td> <td>非必须</td> </tr> <tr> <td>数学模型</td> <td>推理模型+训练算法</td> <td>推理模型</td> </tr> <tr> <td>模型参数</td> <td>机器自动习得</td> <td>人工设计</td> </tr> <tr> <td>准确率</td> <td>理论上达不到 100%</td> <td>很容易达到 100%</td> </tr> <tr> <td>推理</td> <td>计算机</td> <td>计算机</td> </tr> </tbody> </table> <div class="admonition note"> <p class=admonition-title>Note</p> <p>这里强调的"推理模型+训练算法"实际上是强调了机器学习类可以自行更新和加强参数, 通过推理模型根据输入的 <span class=arithmatex>\(x\)</span> 输出 <span class=arithmatex>\(y'\)</span>, 而训练算法则根据 <span class=arithmatex>\(y\)</span> 和 <span class=arithmatex>\(y'\)</span> 的关系不断更新推理模型的参数. 非机器学习类在人工写好模型后就不会再发生改变.</p> </div> <ul> <li>练习: 列出的几种算法中哪一种属于机器学习?</li> </ul> <p>(1), 剪枝算法; (2), 深度优先算法; (3), 蒙特卡洛树搜索; (4), 启发式搜索; (5), 蚁群算法; (6), 宽度优先算法; (7), 最大-最小算法; (8), 线性回归</p> <div class="admonition success"> <p class=admonition-title>参考答案</p> <p>(8). 线性回归的推理模型是 <span class=arithmatex>\(y=bx+a\)</span>, 使用最小二乘法等最小化误差的方式作为训练算法. 对于其他几项, 和机器学习类 AI 比较接近的都是启发式算法. 要么就是传统的图算法. </p> <p>首先对于剪枝算法, 深度优先搜索算法, 宽度优先搜索算法, 最大最小搜索, 这几个都是图或者树的算法, 我们不需要进行训练, 一旦写死了代码其运行方式就不会改变.</p> <p>其次对于蚁群算法, 蒙特卡洛树算法和最大最小搜索算法, 虽然有了随机化模拟, 但是不会更新模型的内部参数, 因此也不属于机器学习的范畴.</p> </div> <ul> <li>机器学习的五个要素: 数据, 模型, 训练, 预测, 评估</li> <li>机器学习的三个核心要素: 数据, 模型, 算法. 机器学习过程: 训练, 预测, 评估</li> <li>深度学习的三个要素: 数据, 算法, 算力</li> </ul> <p>实际上就是我们获得机器学习类模型的几个动作.</p> <p>首先应当获取可靠的数据集, 接下来确定我们使用的模型, 而后使用数据集训练模型调整参数, 将训练后的模型用于新数据的预测, 最后根据预测的准确性和其他性能评估模型.</p> <h3 id=22-2-2>2.2 2. 2 过拟合<a class=headerlink href=#22-2-2 title="Permanent link">#</a></h3> <div class="admonition question"> <p class=admonition-title>Question</p> <p>训练集的结果是不是越准确越好?</p> </div> <div class="admonition question"> <p class=admonition-title>Question</p> <p>为什么我们必须要分出训练集和测试集?</p> </div> <p>答: 不是越准确越好. 我们以最简单的线性回归模型举例, 假设存在一组数据与对应的映射</p> <div class=arithmatex>\[ X:\{1,2,3,4,5,6,7,8,9\}\longrightarrow Y:\{2,4,6,8,7,12,14,16,18\} \]</div> <p>我们很容易观察到, <span class=arithmatex>\(Y\)</span> 与 <span class=arithmatex>\(X\)</span> 的映射关系应当是 <span class=arithmatex>\(Y=2X\)</span>, 但是当我们训练过度之后, 在 <span class=arithmatex>\(X=5\)</span> 的时候模型很可能输出一个接近于 <span class=arithmatex>\(7\)</span> 的数值. 我们称这种<strong>在训练集上表现很好, 预测时表现很差</strong>的现象称之为<strong>过拟合</strong>. </p> <p>解决过拟合问题的最佳方式就是<strong>对收集到的数据进行预处理</strong>, 类似于我们处理实验数据的方式, 将明显和总体趋势相悖的数据除掉, 确保训练数据的质量. 同时监督学习应当分离测试集和训练集.</p> <div class="admonition quote"> <p class=admonition-title>Quote</p> <p><strong>没有免费午餐定理</strong>(NFL): 没有任何机器学习模型在所有问题上都是最优的. 换言之, <strong>一个模型在某些任务上很厉害, 在另外的一些任务上一定会表现很差</strong>. 我们只能尽可能选择合适的数据集和模型进行处理.</p> <p>例如, 如果按照上面我们解决过拟合问题的思路, 除掉 <span class=arithmatex>\(X=5\)</span> 对应的一组数据, 或许可以提高模型整体的泛化能力, 但是在这组带有 <span class=arithmatex>\((x=5, y=7)\)</span> 的测试集数据上的效果就没有过拟合的好.</p> </div> <h3 id=23-2-3>2.3 2. 3 机器学习的分类<a class=headerlink href=#23-2-3 title="Permanent link">#</a></h3> <table> <thead> <tr> <th>学习类别</th> <th>是否存在标签</th> <th>是否存在反馈</th> <th>学习方式</th> <th>实例</th> </tr> </thead> <tbody> <tr> <td>监督学习</td> <td>是</td> <td>是(直接给出正确答案)</td> <td>拟合输入输出关系</td> <td>回归与分类</td> </tr> <tr> <td>无监督学习</td> <td>否</td> <td>否</td> <td>机器自行寻找数据集潜在关系</td> <td>聚类, 异常检测, 降维</td> </tr> <tr> <td><em>半监督学习</em></td> <td>是(少部分)</td> <td>是</td> <td>混合模式</td> <td>-</td> </tr> <tr> <td>强化学习</td> <td>否</td> <td>是(不知道答案, 但是存在奖励机制)</td> <td>不断试错, 不断强化思想钢印, 某种答案最优</td> <td>决策类</td> </tr> </tbody> </table> <p>(ps: 半监督学习了解即可)</p> <h3 id=24-2-4>2.4 2. 4 线性回归与最小二乘法<a class=headerlink href=#24-2-4 title="Permanent link">#</a></h3> <p>线性回归模型是最简单的监督学习模型. 其核心是使误差最小化. 其具体计算原理是将误差的平方和 <span class=arithmatex>\(\sum_{i=1}^{n}e_i^2=\sum_{i=1}^{n}(y_i-(ax_i+b))^2\)</span> 最小化. 如何最小化呢? 答案就是求偏导确定零点, 解这个方程</p> <div class=arithmatex>\[ \left\{\begin{matrix} \frac{\partial f}{\partial a} = 0 \\ \frac{\partial f}{\partial b} = 0 \end{matrix}\right. \]</div> <p>化简即可得到</p> <div class=arithmatex>\[ a=\frac{n\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}x_i\sum_{i=1}^{n}y_i}{n\sum_{i=1}^{n}x_i^2-(\sum_{i=1}^{n}x_i)^2} \]</div> <h2 id=3-chap-iii>3. Chap III 无监督学习: 回归与分类模型<a class=headerlink href=#3-chap-iii title="Permanent link">#</a></h2> <hr> <h3 id=31-3-1>3.1 3. 1 常用损失函数<a class=headerlink href=#31-3-1 title="Permanent link">#</a></h3> <h4 id=311-3-1-1>3.1.1 3. 1. 1 回归损失函数<a class=headerlink href=#311-3-1-1 title="Permanent link">#</a></h4> <p>均方误差(mean squared error). 实际上就是方差.</p> <div class=arithmatex>\[ MSE=\frac{1}{N}\sum_{i=1}^N(Y_i-Y'_i)^2 \]</div> <p>平均绝对误差(mean absolute error).</p> <div class=arithmatex>\[ MAE=\frac{1}{N}\sum_{i=1}^{N}\left|Y_i-Y'_i\right| \]</div> <p>平均绝对百分比误差(mean absolute percentage error).</p> <div class=arithmatex>\[ MAPE=\frac{1}{N}\sum_{i=1}^{N}\left|\frac{Y_i-Y'_i}{Y_i}\right|\times 100 \]</div> <p>均方对数误差(mean squared log error).</p> <div class=arithmatex>\[ MSLE=\frac{1}{N}\sum_{i=1}^{N}(\log \frac{Y'_i+1}{Y_i+1})^2 \]</div> <h4 id=312-3-1-2>3.1.2 3. 1. 2 分类损失函数<a class=headerlink href=#312-3-1-2 title="Permanent link">#</a></h4> <div class="admonition note"> <p class=admonition-title>Note</p> <p>个人认为没必要记. 太难记住了.</p> </div> <h4 id=313-3-1-3>3.1.3 3. 1. 3 一般终止条件<a class=headerlink href=#313-3-1-3 title="Permanent link">#</a></h4> <ul> <li>损失函数值足够小</li> <li>损失函数值的变化足够小</li> <li>训练次数足够多</li> <li>梯度(就是模型参数变化)趋于 0</li> <li>预测集准确率下降</li> </ul> <h3 id=32-3-2>3.2 3. 2 聚类<a class=headerlink href=#32-3-2 title="Permanent link">#</a></h3> <p>属于<strong>无监督学习</strong>, 根据数据点之间特征的<strong>相似度</strong>或距离(欧氏距离)将相似的数据点聚集在一起形成簇.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>欧氏距离</strong>, 线性代数中的概念. 在学习线性代数时我们知道, 欧式空间是线性空间中的一种, n 维空间中的欧氏距离公式是</p> <div class=arithmatex>\[ d(X,Y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2} \]</div> </div> <p>下面我们以典型的 <strong>K-means Algorithm</strong> (<strong>K均值算法</strong>) 为例说明聚类的过程. </p> <p>首先明确最终目的, 将数据集 <span class=arithmatex>\(S\)</span> 按照特征分类为 <span class=arithmatex>\(n\)</span> 组. 我们设 <span class=arithmatex>\(\forall x\in S\)</span> 都对应了 <span class=arithmatex>\(n\)</span> 个特征, <span class=arithmatex>\(S\)</span> 的大小(不同 <span class=arithmatex>\(x\)</span> 的个数)为 <span class=arithmatex>\(N\)</span>. 我们将每个 <span class=arithmatex>\(x\)</span> 的 <span class=arithmatex>\(n\)</span> 个特征映射到一个 <span class=arithmatex>\(n\)</span> 维空间中, 于是我们得到了一个存在 <span class=arithmatex>\(N\)</span> 个点的空间. </p> <p>现在我们假设要将之分为 <span class=arithmatex>\(m\)</span> 组, 那么我们首先随机初始化 <span class=arithmatex>\(m\)</span> 个聚类质心, 然后不断循环下面两步:</p> <ul> <li>计算每个数据到质心的距离, 按照如欧拉距离归类到距离最近的质心, 对应相同质心的点集我们称为簇.</li> <li>调整质心位置, 具体做法是将当前簇内每个点距离之和的(某种)平均值最近的一处作为质心的位置.</li> </ul> <p>直到:</p> <ul> <li>达到迭代次数.</li> <li>前后两次迭代中, 聚类的质心位置基本不变, 趋于稳定.</li> </ul> <p><a class=glightbox data-type=image data-width=auto data-height=auto href=https://files.hollowlib.top/output-(7).6bhbzlxxng.webp data-desc-position=bottom><img alt src=https://files.hollowlib.top/output-(7).6bhbzlxxng.webp></a></p> <p>参考演示网站: <a href=http://alekseynp.com/viz/k-means.html>http://alekseynp.com/viz/k-means.html</a></p> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>KNN</strong> (<strong>最近邻法</strong>), 是一种<strong>监督学习</strong>, 其功能一般是在使用 k-means 聚类之后获取新的输入, 找出与它距离最近的几个样本, 根据最近邻样本的类别的多数判别新输入的类型.</p> </div> <h3 id=33-3-3-pca>3.3 3. 3 降维: 主成分分析(PCA)<a class=headerlink href=#33-3-3-pca title="Permanent link">#</a></h3> <p>目标: 在降维得到的新坐标系中实现数据波动(例如方差)的最大化.</p> <p>视觉上就是建立一个新的坐标系, 将多个维度的数据投影到这个新的坐标系, 实现数据的降维. <strong>新的坐标系轴方向就是主成分</strong>.</p> <p>在代数上, 就是将多个数据分组组合成一个新的特征, 提取最大波动的几个特征作为主特征.</p> <p><a class=glightbox data-type=image data-width=auto data-height=auto href=https://files.hollowlib.top/output-(6).99tm343gpz.webp data-desc-position=bottom><img alt src=https://files.hollowlib.top/output-(6).99tm343gpz.webp></a></p> <h2 id=4-chap-iv-deep-learning>4. Chap IV 深度学习(Deep Learning)导论<a class=headerlink href=#4-chap-iv-deep-learning title="Permanent link">#</a></h2> <hr> <div class="admonition abstract"> <p class=admonition-title>Abstract</p> <p>一个简单的神经元只能完成一个简单的反射. 甚至, 它只能表示 0 和 1. 但是, 当足够多的神经元联结在一起时, 奇妙的事便会发生. </p> <p>本章节中的"深度"是在强调神经网络组织的"深度", 或者说层的数量. 这就是"多层人工神经网络".</p> </div> <h3 id=41-4-1>4.1 4. 1 多层人工神经网络与深度学习<a class=headerlink href=#41-4-1 title="Permanent link">#</a></h3> <div class="admonition note"> <p class=admonition-title>Note</p> <p>深度学习<strong>既不指监督学习或无监督学习, 也不指强化学习</strong>. 其本身是前面几种学习类型, 也就是机器学习的实现方式.</p> </div> <p>深度学习(Deep Learning, 简称 DL)是机器学习的一个子集, 它使用<strong>多层人工神经网络</strong>(特别强调隐藏层的深度)来精准完成图像检测等任务. 通过<strong>多层表示+高阶特征提取</strong>完成如图像识别, 语音识别等各种任务, 具有较强的<strong>特征鲁棒性(抗干扰能力)</strong>.</p> <p><a class=glightbox data-type=image data-width=auto data-height=auto href=https://files.hollowlib.top/output-(8).2h8kgtg188.webp data-desc-position=bottom><img alt src=https://files.hollowlib.top/output-(8).2h8kgtg188.webp></a></p> <p>深度学习的几个特征:</p> <ul> <li>多层网络结构</li> </ul> <p>分为<strong>输入层, 隐藏层, 输出层</strong>. 各层之间通过权重进行连接, 构成复杂的网络结构. 一般而言, 输入层用于"侦测和传递数据", 输出层用于将抽象信息具体化便于我们理解, 隐藏层负责提取特征进行分析.</p> <ul> <li>自动特征提取</li> </ul> <p>自动从原始数据中提取特征, 无需人为提取特征.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>传统的机器学习需要进行人工特征提取. 比如一张图, 首先要人为提取出图片的整体纹理, 颜色直方图, 边缘提取等等, 既耗时又耗力, 而且难以泛化.</p> <p>深度学习通过多个隐藏层从低级特征到特征组合进行学习, 全流程自动化, 不需要人工干预.</p> </div> <ul> <li>非线性激活函数</li> </ul> <p>深度学习模型通常使用非线性激活函数(比如 ReLU, sigmoid 等等)结合多层设计可以轻松处理复杂的非线性关系.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>如果使用线性激活函数, 那么对于每个映射 <span class=arithmatex>\(y=f(x)\)</span>, 都可以写作 <span class=arithmatex>\(y=k_ix+b_i\)</span>. 假设是一个三层神经网络, 则激活函数等价于 <span class=arithmatex>\(y=k_3(k_2(k_1x+b_1)+b_2)+b_3\)</span>, 打开合并后变成 <span class=arithmatex>\(y=Kx+B\)</span>. 也就是说, 如果使用线性激活函数, 再多的层最后都等价于一个层.</p> </div> <ul> <li>大规模数据处理能力</li> </ul> <p>综合前面几个优点, 尤其是自动特征提取, 使用多层人工神经网络的深度学习可以实现这一点.</p> <h3 id=42-4-2>4.2 4. 2 感知机<a class=headerlink href=#42-4-2 title="Permanent link">#</a></h3> <h4 id=421-4-2-1>4.2.1 4. 2. 1 感知机模型<a class=headerlink href=#421-4-2-1 title="Permanent link">#</a></h4> <p>这是经典的人工神经网络模型. 简单来讲, 感知机模拟了一个神经元的行为. 例如, <span class=arithmatex>\(f\)</span> 是一个非线性函数(就是激活函数), 而 <span class=arithmatex>\(X\)</span> 是一个向量矩阵. 这一模型提供一个线性函数, 将 <span class=arithmatex>\(X\)</span> 映射为 <span class=arithmatex>\(\Sigma\)</span>, 其规则表述为 <span class=arithmatex>\(\Sigma=WX^T+B\)</span>. 之后 <span class=arithmatex>\(Y\)</span> 作为非线性函数检测 <span class=arithmatex>\(\Sigma\)</span> 是否可以激活.</p> <div class=arithmatex>\[ Y=f(\Sigma),\ \]</div> <p>在考试中, 会要求我们根据图写出 <span class=arithmatex>\(Y\)</span> 或者 <span class=arithmatex>\(f\)</span> (激活函数)的公式. 对于上述的映射, 我们可以形式化地表述公式为</p> <div class=arithmatex>\[ Y=f(WX^T+B) \]</div> <p>当然, 不适用矩阵也可以表达</p> <div class=arithmatex>\[ Y=f(\sum_{i=1}^{n}(w_ix_i+b)) \]</div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>没有激活函数的感知机本质上是一个线性二分类机器, 无法处理 XOR 类型的问题. 因为 XOR 的结果无法用一条直线分隔开, 所以无解.</p> </div> <h4 id=422-4-2-2-mlp>4.2.2 4. 2. 2 多层感知机(MLP)<a class=headerlink href=#422-4-2-2-mlp title="Permanent link">#</a></h4> <p>多层感知机(Multi-Layer Perceptron, 简称 MLP)的目的是解决非线性问题. 一个多层感知机至少包括三层: 输入层, 隐藏层, 输出层. 这是一个典型的前馈神经网络.</p> <p>我们称只含有一个隐藏层的神经网络叫做<strong>浅层学习网络</strong>, 而将大于一个隐藏层的成为<strong>深度学习网络</strong>.</p> <h4 id=423-4-2-3>4.2.3 4. 2. 3 常见激活函数<a class=headerlink href=#423-4-2-3 title="Permanent link">#</a></h4> <ul> <li>ReLU 函数(线性整流函数), 目前(在隐藏层中)最常用</li> </ul> <div class=arithmatex>\[ f(x)=\max(0,x) \]</div> <ul> <li>Sigmoid 函数, 其只能表述正数(ReLU 可以表述 0), 所以现在已经不常用.</li> </ul> <div class=arithmatex>\[ f(x)=\frac{1}{1+e^{-x}} \]</div> <ul> <li>Softmax 函数, 最常用于分类问题和图像识别等, 其作用是将任意<span class=arithmatex>\(n\)</span> 维实数向量归一化为同阶向量(Softmax 归一化后不但分量全部在 <span class=arithmatex>\([0,1]\)</span>, 而且分量的和也为 <span class=arithmatex>\(1\)</span>).</li> </ul> <div class=arithmatex>\[ f(x_i)=\frac{e^{x_i}}{\sum_{i=1}^{n}e^{x_i}} \]</div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>对于向量 <span class=arithmatex>\(X\)</span> 常用的几种归一化方法(所谓的归一化, 就是让向量的各个分量的取值在 <span class=arithmatex>\([0,1]\)</span> 之间)</p> <p>最小-最大归一化</p> <div class=arithmatex>\[ x'=\frac{x-x_{min}}{x_{max}-x_{min}} \]</div> <p>L2 归一化</p> <div class=arithmatex>\[ x'=\frac{x}{X^2} \]</div> </div> <div class="admonition question"> <p class=admonition-title>Question</p> <p>对于输入 <span class=arithmatex>\(X=[2,0.7,-1.5,-0.9]\)</span>, 计算其 Softmax 输出.</p> </div> <div class="admonition success"> <p class=admonition-title>Success</p> <p>解:</p> <div class=arithmatex>\[ \begin{aligned} &amp;sum=\sum_{i=1}^{n=4}e^{x_i}=e^2+e^{0.7}+e^{-1.5}+e^{-0.9}=10.03\\ &amp;S_1=\frac{e^{x_1}}{sum}=0.74\\ &amp;S_2=\frac{e^{x_2}}{sum}=0.20\\ &amp;S_3=\frac{e^{x_3}}{sum}=0.02\\ &amp;S_4=\frac{e^{x_4}}{sum}=0.04\\ \end{aligned} \]</div> <p>换言之, 其 Softmax 输出为 <span class=arithmatex>\([0.74,0.20,0.02,0.04]\)</span>.</p> </div> <h3 id=43-4-3-bp>4.3 4. 3 反向传播算法: BP<a class=headerlink href=#43-4-3-bp title="Permanent link">#</a></h3> <p>反向传播是人工神经网络用于计算误差的一种手段. 简单来说, 当输出值不在损失允许范围内或根本不符合预期时, 将输出误差以某种形式通过隐藏层向输入层逐层反向传递, 反传递过程中将误差分摊到各个神经网络层, 获取各层单元的误差信号, 算法通过这些误差信号调整连接权值, 从而减小误差, 达到设计预期.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>反向传播算法常用于<strong>前馈神经网络</strong>(FNN)中. 这种神经网络最大的特点就是只会不断向下一层传递, 不像<strong>循环神经网络</strong>(RNN)那样可能会回到前面的隐藏层循环传递.</p> </div> <p>BP 算法的优势 - 自适应与自主学习, 可以自动化地更新隐藏层规则 - 较强的非线性映射能力 - 严谨的推导过程(链式法则) - 较强的泛化能力(通过已有知识解决新问题)</p> <p>BP 算法的劣势 - 容易陷入局部极小值(极小值不一定是最小值) - 收敛速度缓慢(大量参数导数计算和权重与偏置值的更新) - 隐藏层缺少理论指导, 需要不断设计隐藏层和隐藏节点数试凑达到最佳效果 - 学习新样本可能遗忘旧样本(每次更新都是按照新数据进行的, 这种情况很容易发生) - 计算复杂度高</p> <h3 id=44-4-4-gd>4.4 4. 4 梯度下降: GD<a class=headerlink href=#44-4-4-gd title="Permanent link">#</a></h3> <p>在我们<strong>通过反向传播算法得到梯度之后</strong>, 按照公式 <span class=arithmatex>\(w_i\longrightarrow w_i-\eta \times \frac{\partial L}{\partial w_i}\)</span> 不断更新本层的权重和偏置. 其中 <span class=arithmatex>\(\eta\)</span> 是学习率(步长).</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>可能上面的描述会让你感到困惑. 我们其实应当知道, 函数式编程的过程本质就是函数处理常用几种方法的调用. 也就是说, 我们只需要有基本的微积分知识就可以用数学描述上述过程</p> <p>首先, 反向传播算法本质上是对误差函数在各层偏导数的计算. 或者说, 假设某一层的激活函数为 <span class=arithmatex>\(Y_i=f(W_iY_{i-1}^T+B_i)\)</span>, 那么反向传播就是损失函数对 <span class=arithmatex>\(W_i\)</span> 和 <span class=arithmatex>\(B_i\)</span> 求偏导的过程. 通过这一过程, 我们得到了梯度, 也就是偏导数.</p> <p>为了逐步追踪每个参数对于最终损失的间接影响, 我们需要使用链式法则将误差逐层从后向前传递. 比如, 对于一个两层神经网络</p> <div class=arithmatex>\[ \begin{aligned} &amp;a_1=f_1(W_1x+b_1)\\ &amp;a_2=f_2(W_2a_1+b_2)\\ &amp;Loss=\varsigma (a_2,y) \end{aligned} \]</div> <p>其中的 <span class=arithmatex>\(W_1\)</span> 并不会直接影响 <span class=arithmatex>\(Loss\)</span> 函数. 为了计算到其间接影响, 利用多元函数微分法</p> <div class=arithmatex>\[ \frac{\partial \varsigma}{\partial W_1}=\frac{\partial \varsigma}{\partial a_2}\cdot\frac{\partial a_2}{\partial f_2}\cdot\frac{\partial f_2}{\partial a_1}\cdot\frac{\partial a_1}{\partial f_1}\cdot\frac{\partial f_1}{\partial W_1} \]</div> <p>之后的梯度下降实际上是利用反向传播计算得到的结果对所有层同时进行权重 <span class=arithmatex>\(W\)</span> 和偏置 <span class=arithmatex>\(B\)</span> 进行调整的过程.</p> </div> <div class="admonition question"> <p class=admonition-title>Question</p> <p>设函数 <span class=arithmatex>\(Loss=x^2\)</span>, 起点为 <span class=arithmatex>\((8, 64)\)</span>, 学习率为 <span class=arithmatex>\(0.1\)</span>, 使用表格描述其梯度下降的过程.</p> </div> <div class="admonition success"> <p class=admonition-title>Success</p> <p>(好像 admonition 里面打不了表格...)首先求导得到梯度: <span class=arithmatex>\(y=2x\)</span>, 而后逐步计算: <span class=arithmatex>\(y_1=2\times 8=16\)</span>, 因此更新 <span class=arithmatex>\(x_1=x-\eta\cdot y_1=8-0.1\times 16=6.4\)</span>. 类似地, 我们进一步计算 <span class=arithmatex>\(x_2=6.4-0.1\times (6.4\times 2)= 5,12\)</span>, ... 使用图表示为一个梯度下降的过程. <a class=glightbox data-type=image data-width=auto data-height=auto href=https://files.hollowlib.top/output-(6).361u1nkymc.webp data-desc-position=bottom><img alt src=https://files.hollowlib.top/output-(6).361u1nkymc.webp></a></p> </div> <h3 id=45-4-5>4.5 4. 5 梯度下降算法的改进: 优化器<a class=headerlink href=#45-4-5 title="Permanent link">#</a></h3> <h4 id=451-4-5-1-sgd>4.5.1 4. 5. 1 随机梯度下降法(SGD)<a class=headerlink href=#451-4-5-1-sgd title="Permanent link">#</a></h4> <p>传统的梯度下降每次调整都要用训练集中的所有样本, 而随机梯度下降法每次只从训练集中随机选取一个样本, 利用小规模样本训练调整神经网络.</p> <p>其优势有:</p> <ul> <li>高效</li> <li>可并行计算</li> <li>可适应新数据变化(预训练思想萌芽)</li> <li>有机会全局最优</li> </ul> <p>其劣势有:</p> <ul> <li>不稳定</li> <li>没有解决学习率选择问题(需要人工决定学习率)</li> <li>随机最优解</li> <li>模型不可控</li> </ul> <p>改进:</p> <ul> <li>适当增加样例(小批量)</li> <li>动量梯度下降法</li> </ul> <p>相关代码 </p><div class=highlight><table class=highlighttable><tbody><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1>1</a></span>
<span class=normal><a href=#__codelineno-0-2>2</a></span>
<span class=normal><a href=#__codelineno-0-3>3</a></span>
<span class=normal><a href=#__codelineno-0-4>4</a></span>
<span class=normal><a href=#__codelineno-0-5>5</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1></a><span class=c1># optimizer: 优化器, .parameters() 用于读取训练参数, lr 表示 learning rate</span>
<a id=__codelineno-0-2 name=__codelineno-0-2></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
<a id=__codelineno-0-3 name=__codelineno-0-3></a>
<a id=__codelineno-0-4 name=__codelineno-0-4></a><span class=c1># momentum: 动量, 保留上一次梯度 90% 的值, 类似于惯性</span>
<a id=__codelineno-0-5 name=__codelineno-0-5></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</code></pre></div></td></tr></tbody></table></div><p></p> <h4 id=452-4-5-2-adagrad>4.5.2 4. 5. 2 自适应梯度算法(AdaGrad)<a class=headerlink href=#452-4-5-2-adagrad title="Permanent link">#</a></h4> <p>自适应梯度算法(AutoGrad)通过自动调整学习率实现在不同梯度尺度中稳健的表现. 其学习率的分母部分会增加历史梯度的累积值, 这意味着其学习率会随着训练次数的增加逐渐趋于 0.</p> <p>其优势有:</p> <ul> <li>自动化调整学习率</li> <li>权重的"步调一致", 实现对典型特征(高频特征, 高学习率)的提取, 适当减少对稀疏特征(低频特征, 低学习率)的提取, 避免过拟合</li> </ul> <p>其劣势有:</p> <ul> <li>梯度消失(学习率趋于零)</li> <li>训练速度缓慢(原因与上面一样, 学习率会逐渐减小)</li> </ul> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>稀疏数据</strong>是指维度很高但是绝大多数方向上的值是 0 的数据. </p> </div> <p>因为 AdaGrad 的设计原理问题, 实际上的原始算法只适用于稀疏数据. 像图像等含有大量低维数据, 需要长时间进行神经网络训练的不适合 AdaGrad.</p> <p>改进: - RMSProp(自适应平方根梯度法): 调整分母的值 - Adam(自适应矩估计法, 目前最常见): 本质上是 RMSProp + Momentum</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Adam 结合了两种优化思路, 其核心是为每个参数维护两个动量, 一个是一阶动量估计(梯度指数的加权平均), 另一个是二阶动量估计(梯度平方的指数加权平均). 如果想要详细了解, 可以看看这篇论文<a href=https://arxiv.org/pdf/1412.6980>https://arxiv.org/pdf/1412.6980</a>.</p> </div> <p>相关代码 </p><div class=highlight><table class=highlighttable><tbody><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-1-1> 1</a></span>
<span class=normal><a href=#__codelineno-1-2> 2</a></span>
<span class=normal><a href=#__codelineno-1-3> 3</a></span>
<span class=normal><a href=#__codelineno-1-4> 4</a></span>
<span class=normal><a href=#__codelineno-1-5> 5</a></span>
<span class=normal><a href=#__codelineno-1-6> 6</a></span>
<span class=normal><a href=#__codelineno-1-7> 7</a></span>
<span class=normal><a href=#__codelineno-1-8> 8</a></span>
<span class=normal><a href=#__codelineno-1-9> 9</a></span>
<span class=normal><a href=#__codelineno-1-10>10</a></span>
<span class=normal><a href=#__codelineno-1-11>11</a></span>
<span class=normal><a href=#__codelineno-1-12>12</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1></a><span class=c1># weight_decay: 正则化项(L2 惩罚), 防止过拟合; eps 用于替代分母等于 0 的项</span>
<a id=__codelineno-1-2 name=__codelineno-1-2></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adagrad</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
<a id=__codelineno-1-3 name=__codelineno-1-3></a>                          <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
<a id=__codelineno-1-4 name=__codelineno-1-4></a>                          <span class=n>weight_decay</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>,</span>
<a id=__codelineno-1-5 name=__codelineno-1-5></a>                          <span class=n>eps</span><span class=o>=</span><span class=mf>1e-10</span><span class=p>)</span>
<a id=__codelineno-1-6 name=__codelineno-1-6></a>
<a id=__codelineno-1-7 name=__codelineno-1-7></a><span class=c1># betas: 动量因子, beta1控制一阶滑动平均, beta2控制二阶平方平均</span>
<a id=__codelineno-1-8 name=__codelineno-1-8></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
<a id=__codelineno-1-9 name=__codelineno-1-9></a>                       <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
<a id=__codelineno-1-10 name=__codelineno-1-10></a>                       <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>),</span>
<a id=__codelineno-1-11 name=__codelineno-1-11></a>                       <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>,</span>
<a id=__codelineno-1-12 name=__codelineno-1-12></a>                       <span class=n>weight_decay</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div></td></tr></tbody></table></div><p></p> <h2 id=5>5. 幕间: 关系总结<a class=headerlink href=#5 title="Permanent link">#</a></h2> <hr> <p>人工智能分为两大类, 一类是机器学习, 另一类是非机器学习. 前者除了模型本身, 还有训练算法, 可以不断增强自己的参数. 后者基本属于传统算法范畴, 在本门课程基本不做讨论.</p> <p>对于机器学习, 我们可以从模型设计和训练算法两方面来分类. </p> <p>对于训练算法, 我们分为监督学习, 无监督学习和强化学习, 回归是典型的监督学习, 聚类和降维则是典型的无监督学习. </p> <p>对于模型设计, 我们主要讨论的是深度学习. 深度学习利用深层神经网络, 实现层级的特征抽象提取和表示学习. 现在最有名的基于 Transformer 架构的各类大模型, 就是典型的深度学习模型.</p> <p>如果你是一位资深的 AI 用户, 你会发现几乎所有的 AI 都可以做到和你对话. 这一点归功于<strong>预训练模型</strong>. 预训练模型在深度学习模型的基础上, 使用大数据得到基本通用知识, 再迁移到各种具体任务中使用专业数据集进一步训练.</p> <p>而当预训练模型被调教好后, 我们就得到了<strong>生成式大模型</strong>. 生成式大模型就是根据上下文自动生成相对高质量内容的<strong>大规模人工神经网络</strong>. 这其中我们最为熟悉, 最常用的便是<strong>生成式大语言模型</strong>.</p> <h2 id=6-chap-v>6. Chap V 深度学习实例<a class=headerlink href=#6-chap-v title="Permanent link">#</a></h2> <hr> <h3 id=61-5-1-cnn>6.1 5. 1 CNN 卷积神经网络<a class=headerlink href=#61-5-1-cnn title="Permanent link">#</a></h3> <p>卷积神经网络(CNN)是一类具有局部感受野, 权值共享机制和多层结构的前馈神经网络, 主要用于处理具有网格的数据(例如图像, 语音, 视频等), 尤其擅长从原始输入中自动提取空间和时序特征.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>感受野</strong>, 指神经网络某个神经元在输入图像时可以"看到"的区域大小. 例如我们人类只能准确看到在我们实现中央的区域, 我们可以粗略的将我们的感受野归结为中间区域.</p> </div> <p>卷积神经网络相较于普通的人工神经网络而言, 多了卷积层和池化层结构. 我们在卷积神经网络中一般简称传统人工神经网络组成部分(包括输入层, 隐藏层, 输出层)为全连接层.</p> <h4 id=611-5-1-1>6.1.1 5. 1. 1 卷积运算<a class=headerlink href=#611-5-1-1 title="Permanent link">#</a></h4> <p>设计一系列大小合适的卷积核(感受野), 对数字图像的各个通道分量进行卷积. 或者说, 一个卷积核按照步长对于图像全部像素进行加权求和, 卷积核中存储有权重值. 常用的卷积核大小有 <span class=arithmatex>\(3\times3,5\times5,7\times7\)</span>.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>人工智能领域的卷积运算一般是按照卷积核确定的权重进行加权求和. 其加权计算过程如图所示 <a class=glightbox data-type=image data-width=auto data-height=auto href=https://files.hollowlib.top/%E5%8F%8D%E8%89%B2_%E5%8D%B7%E7%A7%AF%E7%A4%BA%E6%84%8F_%E6%B8%85%E6%99%B0%E7%89%88.7snh2lmf2v.webp data-desc-position=bottom><img alt src=https://files.hollowlib.top/%E5%8F%8D%E8%89%B2_%E5%8D%B7%E7%A7%AF%E7%A4%BA%E6%84%8F_%E6%B8%85%E6%99%B0%E7%89%88.7snh2lmf2v.webp></a></p> </div> <p>用于卷积运算的神经网络层成为卷积层.</p> <p>卷积运算中有三个重要参数, 分别是<strong>卷积核的形状</strong>, <strong>卷积的步长</strong>, <strong>卷积核的个数</strong>(每一层卷积层可能不同).</p> <h4 id=612-5-1-2>6.1.2 5. 1. 2 池化<a class=headerlink href=#612-5-1-2 title="Permanent link">#</a></h4> <p>池化也称为下采样. 其作用是进一步缩小特征图的尺寸, 减小计算量. 池化的原理是可以利用某一图像区域子块的统计信息包含该子块的全局信息. CNN 通常使用 <span class=arithmatex>\(2\times 2\)</span> 区域进行池化.</p> <p>一般池化分为四种, 分别是<strong>最大池化</strong>, <strong>随机池化</strong>, <strong>平均池化</strong>, <strong>L2 范数池化</strong>. 前面三种顾名思义, 第四种以 CNN 为例, 设 <span class=arithmatex>\(S(0,0)=19, S(0,1)=15, S(1,0)=26, S(1,1)=23\)</span>, 那么其 L2范数就是 <span class=arithmatex>\(\sqrt{19^2+15^2+26^2+23^2}\)</span>.</p> <h4 id=613-5-1-3>6.1.3 5. 1. 3 本质<a class=headerlink href=#613-5-1-3 title="Permanent link">#</a></h4> <ul> <li>独热码</li> </ul> <p>CNN 通过<strong>独热码</strong>(One-Hot Code)来表示物质的性质. 在分类中, 我们不能简单的使用 1, 2, 3, 4 ... 这种有序数字来表达. 否则假设猪编码为 1, 羊编码为 2, 鸡编码为 3, 那么就会出现"鸡-羊=1"这种毫无意义的数值关系.</p> <p>独热码将猪, 羊, 鸡按照下面这种方式进行编码:</p> <table> <thead> <tr> <th>类别</th> <th>猫</th> <th>羊</th> <th>猪</th> <th>狗</th> <th>鸡</th> <th>鸭</th> </tr> </thead> <tbody> <tr> <td>猪</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>羊</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>鸡</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> </tbody> </table> <p>在最终输出的时候将独热码的值进行 Softmax 计算, 得到是某一种物种的概率.</p> <ul> <li>层次特征学习</li> </ul> <p>深度神经网络(指隐藏层大于 1)可以通过层次性处理的方式逐步提取出抽象特征, 并且随着层次的深入, 低级特征逐渐向高级特征进化.</p> <h4 id=614-5-1-4>6.1.4 5. 1. 4 计算<a class=headerlink href=#614-5-1-4 title="Permanent link">#</a></h4> <p>一般而言, 会要求你已知输入的情况下给出输出. 使用一维的情况进行记忆: </p> <p>假设输入的长度为(数目) <span class=arithmatex>\(I\times1\)</span>, 卷积核大小(kernel_size)为 <span class=arithmatex>\(K\times1\)</span>, 填充大小(padding)为 <span class=arithmatex>\(P\)</span> (在被卷积的输入两边都添加 <span class=arithmatex>\(P\)</span> 个 <span class=arithmatex>\(0\)</span> 控制卷积的), 步长为 <span class=arithmatex>\(S\)</span>. 那么, 我们按照以下操作进行计算:</p> <p>首先在原有的输入两边加上 <span class=arithmatex>\(P\)</span> 个 <span class=arithmatex>\(0\)</span>, 输入的长度变为 <span class=arithmatex>\(I+2\times P\)</span>, 而后我们使用卷积核(<span class=arithmatex>\(K\times 1\)</span>)按照步长(<span class=arithmatex>\(S\)</span>)在输入上移动. 我们知道, 假设 <span class=arithmatex>\(S=1\)</span> 的时候我们最终可以得到 <span class=arithmatex>\(I+2\times P - K + 1\)</span> 个完整的卷积输出, 而在 <span class=arithmatex>\(S=2\)</span> 的时候可以得到 <span class=arithmatex>\(\frac{I+2\times P-K}{S}+1\)</span> 个完整输出. 利用数学归纳法可以确定最终公式为</p> <div class=arithmatex>\[ Output=floor(\frac{Input+Padding\times 2-KernelSize}{Stride})+1 \]</div> <p>最终输出还应该加上卷积核, 也就是通道的数目. 例如, 这一层卷积核有 10 个, 那么通道就有 10 个.</p> <div class="admonition question"> <p class=admonition-title>Question</p> <p>对 kernels = <span class=arithmatex>\(96\)</span>, kernel_size = <span class=arithmatex>\(11\times11\)</span>, padding = <span class=arithmatex>\([1,2]\)</span>, stride = <span class=arithmatex>\(4\)</span>, input = <span class=arithmatex>\([224,224,3]\)</span> 的图像进行卷积计算, 计算出输出形状.<sup id=fnref:1><a class=footnote-ref href=#fn:1>1</a></sup></p> </div> <div class="admonition success"> <p class=admonition-title>Success</p> <p><span class=arithmatex>\([55,55,96]\)</span>, 按公式计算即可. 注意这里一般将<strong>张量</strong>描述为<strong>形状</strong>, 其要素为<span class=arithmatex>\([高度(H),宽度(W),通道数(C)]\)</span>.</p> </div> <h3 id=62>6.2 幕间: 三种神经网络<a class=headerlink href=#62 title="Permanent link">#</a></h3> <p>神经网络分为三类, 有<strong>前馈神经网络</strong>, <strong>反馈神经网络</strong>, 以及<strong>图网络</strong>. </p> <ul> <li>前馈神经网络</li> </ul> <p>前面我们学的两个, 分别是 <strong>MLP</strong> <strong>多层感知机</strong>, <strong>CNN</strong> <strong>卷积神经网络</strong>.</p> <ul> <li>反馈神经网络</li> </ul> <p>常见的如 <strong>RNN</strong> <strong>循环神经网络</strong>, <strong>LSTM</strong> <strong>长短程记忆网络</strong>, <strong>Hopfield</strong> <strong>网络</strong>, <strong>玻尔兹曼机</strong></p> <ul> <li>图网络</li> </ul> <p><strong>知识图谱</strong>, <strong>社交网络</strong>, <strong>城市交通</strong>等特征为关系的结构.</p> <h3 id=63-5-2-rnn>6.3 5. 2 RNN 循环神经网络<a class=headerlink href=#63-5-2-rnn title="Permanent link">#</a></h3> <p>循环神经网络(RNN), 其特点是可以处理顺序性信息, 或者说具有时序性的信息. 具体而言, RNN 在每个时间步除了像一般的神经网络那样处理输入输出, 还会更新其内部的隐藏状态提供给下一个时间步. 这意味着每个时间步的输出不但依赖本次输入, 还依赖于上一次结束时的记录的状态, 并且会继续记录下去.</p> <div class="admonition question"> <p class=admonition-title>Question</p> <p>已知拓扑结构为同步多对多的 RNN, 输入层, 隐含层和输出层的神经元都只有一个, 激活函数均为 ReLU. 已知 <span class=arithmatex>\(W=[0.5,0.1,0.2]\)</span>, <span class=arithmatex>\(H=[1]\)</span>, <span class=arithmatex>\(V=[3]\)</span>, <span class=arithmatex>\(S_0=0\)</span>, <span class=arithmatex>\(\alpha = 0\)</span>, <span class=arithmatex>\(\beta=0\)</span>, 对输入序列 <span class=arithmatex>\(X=[[1,1,1],[2,2,2],[3,3,3]]\)</span>, 计算其对应的输出序列. 其中 <span class=arithmatex>\(Y_t=h(V\cdot S_t+\alpha)\)</span>, <span class=arithmatex>\(S_t=f(W\cdot X_t+H\cdot S_{t-1}+\beta)\)</span>.</p> </div> <div class="admonition success"> <p class=admonition-title>Success</p> <p>虽然看起来很麻烦, 但其实计算相当简单</p> <div class=arithmatex>\[ \begin{aligned} &amp;S_1=f([0.5,0.1,0.2]\cdot [1,1,1]+1\cdot 0+0)=f(0.8),\ 0.8&gt;0,\ S_1=0.8\\ &amp;Y_1=h(3\cdot 0.8+0)=h(2.4),\ 2.4&gt;0,\ Y_1=2.4\\ &amp;S_2=f([0.5,0.1,0.2]\cdot [2,2,2]+1\cdot 0.8)=f(2.4)=2.4\\ &amp;Y_2=h(3\cdot 2.4+0)=h(7.2)=7.2\\ &amp;...(the\ same\ way) \end{aligned} \]</div> </div> <h3 id=64-5-3-lstm>6.4 5. 3 LSTM 长短程记忆网络<a class=headerlink href=#64-5-3-lstm title="Permanent link">#</a></h3> <p>传统的 RNN 无法很好的处理长期记忆的问题, 而 LSTM 引入了记忆状态, 利用门控循环单元(GRU)控制信息的读写, 可以很好的决定是否长时间保存记忆.</p> <p>当然, LSTM 并不是完美无缺的, 它因为引入了额外的部件, 其计算变得极其复杂, 需要花费大量时间, 无法进行实时响应, 并且只能顺序处理信息.</p> <h2 id=7-chap-vi-nlp>7. Chap VI 自然语言处理(NLP)<a class=headerlink href=#7-chap-vi-nlp title="Permanent link">#</a></h2> <hr> <h3 id=71-6-1>7.1 6. 1 历史<a class=headerlink href=#71-6-1 title="Permanent link">#</a></h3> <p>NLP 是人工智能领域的掌上明珠, 同时也是许多人投身于人工智能的最终目标. 其历史不可谓不坎坷, 历经了如下的演变</p> <ul> <li>基于规则算法(语法解析器, 类似于程序语言的编译器)</li> <li>统计语言模型(例如 CBOW(Continuous Bag-of-Words), Skip-Gram等等, 根据语义相似度推断)</li> <li>序列生成模型(例如 RNN, Transformer. 前者缺少长距离记忆能力, 计算无法并行; 而后者不但支持长距离全局记忆能力, 而且可以并行计算)</li> <li>预训练-微调模型(一般首先进行无监督(或者自监督)预训练, 而后确定任务对预训练大模型进行微调(一般是监督学习))</li> </ul> <div class="admonition note"> <p class=admonition-title>Note</p> <p>CBOW 和 Skip-Gram 是两个截然处理顺序相反的模型. 前者是在已知 <span class=arithmatex>\(a_{i-2}, a_{i-1}, a_{i+1}, a_{i+2}\)</span> 的前提下预测 <span class=arithmatex>\(a_i\)</span>, 后者是在已知 <span class=arithmatex>\(a_i\)</span> 的前提下预测其他几个.</p> </div> <h3 id=72-6-2>7.2 6. 2 分词<a class=headerlink href=#72-6-2 title="Permanent link">#</a></h3> <p>将一句连续的自然语言转换为一系列的 <strong>token (令牌)</strong> . Token 可以是一个字节, 一个字, 也可以是一个词. 将一句话分割为 Token 这种最小单元, 保证自然语言处理模型可以处理.</p> <p>例如, 我们将"黄山落叶松叶落山黄"进行按字分词: 黄-山-叶-落-松-叶-落-山-黄, 词汇表(不同 token)有 5 个 token, 句子共有 9 个 token. 按词分: 黄山-落叶松-叶-落-山-黄, 词汇表有 6 个 token, 句子有 6 个 token.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>词向量</strong>和<strong>词嵌入</strong>都是一个东西, 区别在于词向量强调的是数字编码技术, 词嵌入强调的是 NLP 网络之间的数据存在形式. 注意, 我们对词向量基本只关注其相对方向, 因此其分量没有真正的限制范围</p> </div> <p>现代句子在分词后的表示一般采用一维的稠密矩阵而非二维稀疏矩阵(独热码), 因为以为稠密矩阵的存储效率高很多.</p> <h3 id=73-6-3>7.3 6. 3 文本相似度计算<a class=headerlink href=#73-6-3 title="Permanent link">#</a></h3> <p>成功地完成了词向量的表示之后, 我们就可以很轻松地计算文本相似度. 一般计算方式有以下几种</p> <ul> <li>余弦相似度</li> </ul> <div class=arithmatex>\[ Cosine\ Similarity= \cos\theta=\frac{A\cdot B}{\left|A\right|\left|B\right|} \]</div> <ul> <li>曼哈顿距离</li> </ul> <div class=arithmatex>\[ d=\sqrt{\sum_{i=1}^{n}\left|X_i-Y_i\right|} \]</div> <ul> <li>欧氏距离</li> </ul> <div class=arithmatex>\[ d=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2} \]</div> <ul> <li>Jaccard 相似度</li> </ul> <p>第一定义</p> <div class=arithmatex>\[ E(A,B)=\frac{A\cdot B}{A^2+B^2-A\cdot B} \]</div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>第一定义其实是很有意思的. 其分母与狭义定义比较, 其实是并集的计算公式: 二者之和减去相同的部分, 也就是 <span class=arithmatex>\(\left|A\cup B\right|=\left|A\right|+\left|B\right|-\left|A\cap B\right|\)</span></p> </div> <p>狭义定义</p> <div class=arithmatex>\[ E=\frac{\left|A\cap B\right|}{\left|A\cup B\right|} \]</div> <div class="admonition question"> <p class=admonition-title>Question</p> <p>根据图中提供的信息计算华为和苹果的余弦相似度与广义 Jaccard 相似度.</p> </div> <div class="admonition success"> <p class=admonition-title>Success</p> <p>我们首先提取华为和苹果的词向量. 华为: <span class=arithmatex>\(A=[0.02, 0.93, 0.95, 0.01]\)</span>, 苹果: <span class=arithmatex>\(B=[0.96,0.77,0.85,0.15]\)</span>. 因此根据余弦相似度的计算公式, 我们得到</p> <div class=arithmatex>\[ COS(A,B)=\frac{A\cdot B}{\left|A\right|\left|B\right|}=0.7727 \]</div> <p>对于广义 Jaccard 相似度</p> <div class=arithmatex>\[ J(A,B)=\frac{A\cdot B}{A^2+B^2-A\cdot B}=0.6219 \]</div> </div> <div class="admonition question"> <p class=admonition-title>Question</p> <p>考虑以下两个文本的 Jaccard 相似度. 文本 1: "我爱天安门", 文本 2: "天安门雄伟壮阔让人不得不爱".(不考虑词频)</p> </div> <div class="admonition success"> <p class=admonition-title>Success</p> <p>文本 1 集合为 <span class=arithmatex>\(A=\)</span> {我,爱,天,安,门}, 文本 2 集合为 <span class=arithmatex>\(B=\)</span> {天,安,门,雄,伟,壮,阔,让,人,不,得,爱}. 因此我们找到交集: {爱, 天, 安, 门}, 找到并集数目: 根据公式 <span class=arithmatex>\(A\cup B=A+B-A\cap B\)</span> 知并集的数目为(注意写集合时排除重复元素"不")13. 最终计算结果为 <span class=arithmatex>\(\frac{4}{13}\)</span>.</p> </div> <div class="admonition note"> <p class=admonition-title>Note</p> <p><strong>词袋模型</strong>, 是一种自然语言处理和信息检索中的常用文本表示方法. 它将文本表示为一个词的集合, 忽略顺序和语法结构, 只关注词语的出现频率或其他统计量.</p> </div> <h3 id=74-6-4-transformer>7.4 6. 4 现代自然语言处理框架: Transformer<a class=headerlink href=#74-6-4-transformer title="Permanent link">#</a></h3> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>此部分内容我应该会在暑假继续更新. 多留点空以便更新. 因为这部分前沿内容比较复杂, 很难学懂和实践, 所以考的基本上都是名词理解.</p> </div> <h4 id=741-6-4-1>7.4.1 6. 4. 1 简述<a class=headerlink href=#741-6-4-1 title="Permanent link">#</a></h4> <p>推荐某乎上的这篇文章: <a href=https://zhuanlan.zhihu.com/p/338817680>Transformer模型详解（图解最完整版） - 初识CV的文章 - 知乎</a>, 这里因为时间原因不会讲的很细.</p> <p>Transformer 架构由编码器和解码器两部分组成. 其工作流程大致如下:</p> <ul> <li>输入一个句子, 获取句子中的每个单词的表示向量 <span class=arithmatex>\(X\)</span>, <span class=arithmatex>\(X\)</span> 由单词的词嵌入(Words Embedding)和单词位置的 Embedding 相加得到.</li> <li>将得到的单词表示为向量矩阵, 将矩阵传入到编码器中, 经过处理后得到所有单词的编码信息矩阵 <span class=arithmatex>\(C\)</span></li> <li>将矩阵 <span class=arithmatex>\(C\)</span> 传入到解码器中, <strong>解码器</strong>会通过前面 <span class=arithmatex>\(i\)</span> 个词翻译第 <span class=arithmatex>\(i+1\)</span> 个词, 同时会掩蔽 <span class=arithmatex>\(i+1\)</span> 后面的所有词.</li> </ul> <div class="admonition note"> <p class=admonition-title>Note</p> <p>在 Transformer 架构下的编码器 BERT 不会掩蔽掉后面的值, 但是 解码器 GPT 会掩蔽掉.</p> </div> <h4 id=742-6-4-2>7.4.2 6. 4. 2 自注意力机制<a class=headerlink href=#742-6-4-2 title="Permanent link">#</a></h4> <p>一句话可以概括为: 看某个词的时候也在看其他所有词. 该地方输出的是结合了所有信息, 只不过赋予不同权重的向量.</p> <h4 id=743-6-4-3>7.4.3 6. 4. 3 三种模型<a class=headerlink href=#743-6-4-3 title="Permanent link">#</a></h4> <table> <thead> <tr> <th>模型</th> <th>采用编码器</th> <th>采用解码器</th> </tr> </thead> <tbody> <tr> <td>BERT</td> <td>TRUE</td> <td>FALSE</td> </tr> <tr> <td>GPT</td> <td>FALSE</td> <td>TRUE</td> </tr> <tr> <td>T5</td> <td>TRUE</td> <td>TRUE</td> </tr> </tbody> </table> <h2 id=8-chap-vii-aigc-llm>8. Chap VII AIGC 与 LLM<a class=headerlink href=#8-chap-vii-aigc-llm title="Permanent link">#</a></h2> <hr> <h3 id=81-7-1>8.1 7. 1 绪论<a class=headerlink href=#81-7-1 title="Permanent link">#</a></h3> <p>AIGC(AI Generated Content), 这些年发展速度之快已经远超大家的想象. 甚至有的人已经怀疑现在基于 LLM 的 AI 在某种程度上已经可以称为真正的"智能"了.</p> <ul> <li>区别: 传统搜索引擎与 AIGC</li> </ul> <p>AIGC 会生成全新的内容, 而传统搜索引擎做不到.</p> <ul> <li>AIGC 的局限性</li> </ul> <p>原创性几乎为 0, 可解释性奇差无比(概率模型, 没有明确的因果链), 语义理解长度有限, 长文本内容和大幅度时间跨度难以解析.</p> <h3 id=82-7-2>8.2 7. 2 名词解释<a class=headerlink href=#82-7-2 title="Permanent link">#</a></h3> <ul> <li>LLM: 大语言模型(Large Language Model), 其特征表现为训练数据大, 参数规模大, 耗资巨大. </li> <li>涌现能力: 当一种系统在复杂度增加到某一临界点时, 会出现其子系统或小规模版本中未曾存在的行为或特性.</li> <li>GAI: 生成式人工智能(Generated AI), 指生成全新内容的 AI. 需要注意的是, GAI 采用自回归生成技术, 有随机性(同一提示文本不同回答); 同时 GAI 不是搜索引擎(虽然很多人这么用就是了).</li> <li>AGI: 通用人工智能(Artificial General Intelligence), 指能够完成人类能够完成的任何智力任务. 其最大的特点是训练"领域无关", "任务无关".</li> <li>GPT: 基于 Transformer 的预训练生成式模型(Generative Pre-Trained Transformer)</li> <li>Chat GPT: 采用 GPT 架构的聊天机器人产品.</li> <li>CLIP: 即视觉语言预训练模型(Constructive Language-Image Pre-training), 由 Open AI 提出的一个跨模态模型, 可以将图像和文本嵌入到同一个向量空间(隐空间)中进行比较和对齐. 其训练方式是用大量的"图对文"数据将图映射成文的向量.</li> <li>扩散模型: 即先加噪声模糊化, 然后让机器学习如何修复图像可以尽可能地使图像接近原本的样子. 分为前向过程("打马赛克", 扩散过程)和反向过程("去马赛克"). U-net 是其中最常用的一个模型.</li> <li>多模态大语言模型(MLLM): 可以给以任意形式的输入, 可以给出任意形式的输出. 这就是最完美的多模态大语言模型.</li> </ul> <h3 id=83-7-3>8.3 7. 3 大语言模型的特征<a class=headerlink href=#83-7-3 title="Permanent link">#</a></h3> <ul> <li>参数规模巨大</li> <li>涌现能力: 随着模型规模提升模型性能显著上升</li> <li>数据驱动</li> <li>端到端学习: 不需要人工特征工程或者规则设计, 让 AI 自行完成学习</li> <li>上下文感知</li> <li>通用性</li> </ul> <h2 id=9-i-python-python>9. 附则 I python 基础与机器学习常用 python 库<a class=headerlink href=#9-i-python-python title="Permanent link">#</a></h2> <hr> <ul> <li>scikit-learn: 深度学习库, 主要分为四种算法: 分类, 回归, 聚类, 降维</li> </ul> <div class=highlight><table class=highlighttable><tbody><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-2-1> 1</a></span>
<span class=normal><a href=#__codelineno-2-2> 2</a></span>
<span class=normal><a href=#__codelineno-2-3> 3</a></span>
<span class=normal><a href=#__codelineno-2-4> 4</a></span>
<span class=normal><a href=#__codelineno-2-5> 5</a></span>
<span class=normal><a href=#__codelineno-2-6> 6</a></span>
<span class=normal><a href=#__codelineno-2-7> 7</a></span>
<span class=normal><a href=#__codelineno-2-8> 8</a></span>
<span class=normal><a href=#__codelineno-2-9> 9</a></span>
<span class=normal><a href=#__codelineno-2-10>10</a></span>
<span class=normal><a href=#__codelineno-2-11>11</a></span>
<span class=normal><a href=#__codelineno-2-12>12</a></span>
<span class=normal><a href=#__codelineno-2-13>13</a></span>
<span class=normal><a href=#__codelineno-2-14>14</a></span>
<span class=normal><a href=#__codelineno-2-15>15</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-2-1 name=__codelineno-2-1></a><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_iris</span>
<a id=__codelineno-2-2 name=__codelineno-2-2></a>
<a id=__codelineno-2-3 name=__codelineno-2-3></a><span class=n>iris</span> <span class=o>=</span> <span class=n>load_iris</span><span class=p>()</span>
<a id=__codelineno-2-4 name=__codelineno-2-4></a><span class=nb>print</span><span class=p>(</span><span class=s2>"鸢尾花数据返回值: </span><span class=se>\n</span><span class=s2>"</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
<a id=__codelineno-2-5 name=__codelineno-2-5></a>
<a id=__codelineno-2-6 name=__codelineno-2-6></a><span class=n>X</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>data</span>
<a id=__codelineno-2-7 name=__codelineno-2-7></a><span class=n>y</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span>
<a id=__codelineno-2-8 name=__codelineno-2-8></a>
<a id=__codelineno-2-9 name=__codelineno-2-9></a><span class=c1># 数据标准化</span>
<a id=__codelineno-2-10 name=__codelineno-2-10></a><span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<a id=__codelineno-2-11 name=__codelineno-2-11></a><span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<a id=__codelineno-2-12 name=__codelineno-2-12></a>
<a id=__codelineno-2-13 name=__codelineno-2-13></a><span class=c1># PCA(主成分降维)</span>
<a id=__codelineno-2-14 name=__codelineno-2-14></a><span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>()</span>
<a id=__codelineno-2-15 name=__codelineno-2-15></a><span class=n>X_pca</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span>
</code></pre></div></td></tr></tbody></table></div> <ul> <li>python 关键字"真"</li> </ul> <div class=highlight><table class=highlighttable><tbody><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-3-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-3-1 name=__codelineno-3-1></a><span class=kc>True</span>
</code></pre></div></td></tr></tbody></table></div> <ul> <li>Tensorflow </li> </ul> <div class=highlight><table class=highlighttable><tbody><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-4-1> 1</a></span>
<span class=normal><a href=#__codelineno-4-2> 2</a></span>
<span class=normal><a href=#__codelineno-4-3> 3</a></span>
<span class=normal><a href=#__codelineno-4-4> 4</a></span>
<span class=normal><a href=#__codelineno-4-5> 5</a></span>
<span class=normal><a href=#__codelineno-4-6> 6</a></span>
<span class=normal><a href=#__codelineno-4-7> 7</a></span>
<span class=normal><a href=#__codelineno-4-8> 8</a></span>
<span class=normal><a href=#__codelineno-4-9> 9</a></span>
<span class=normal><a href=#__codelineno-4-10>10</a></span>
<span class=normal><a href=#__codelineno-4-11>11</a></span>
<span class=normal><a href=#__codelineno-4-12>12</a></span>
<span class=normal><a href=#__codelineno-4-13>13</a></span>
<span class=normal><a href=#__codelineno-4-14>14</a></span>
<span class=normal><a href=#__codelineno-4-15>15</a></span>
<span class=normal><a href=#__codelineno-4-16>16</a></span>
<span class=normal><a href=#__codelineno-4-17>17</a></span>
<span class=normal><a href=#__codelineno-4-18>18</a></span>
<span class=normal><a href=#__codelineno-4-19>19</a></span>
<span class=normal><a href=#__codelineno-4-20>20</a></span>
<span class=normal><a href=#__codelineno-4-21>21</a></span>
<span class=normal><a href=#__codelineno-4-22>22</a></span>
<span class=normal><a href=#__codelineno-4-23>23</a></span>
<span class=normal><a href=#__codelineno-4-24>24</a></span>
<span class=normal><a href=#__codelineno-4-25>25</a></span>
<span class=normal><a href=#__codelineno-4-26>26</a></span>
<span class=normal><a href=#__codelineno-4-27>27</a></span>
<span class=normal><a href=#__codelineno-4-28>28</a></span>
<span class=normal><a href=#__codelineno-4-29>29</a></span>
<span class=normal><a href=#__codelineno-4-30>30</a></span>
<span class=normal><a href=#__codelineno-4-31>31</a></span>
<span class=normal><a href=#__codelineno-4-32>32</a></span>
<span class=normal><a href=#__codelineno-4-33>33</a></span>
<span class=normal><a href=#__codelineno-4-34>34</a></span>
<span class=normal><a href=#__codelineno-4-35>35</a></span>
<span class=normal><a href=#__codelineno-4-36>36</a></span>
<span class=normal><a href=#__codelineno-4-37>37</a></span>
<span class=normal><a href=#__codelineno-4-38>38</a></span>
<span class=normal><a href=#__codelineno-4-39>39</a></span>
<span class=normal><a href=#__codelineno-4-40>40</a></span>
<span class=normal><a href=#__codelineno-4-41>41</a></span>
<span class=normal><a href=#__codelineno-4-42>42</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-4-1 name=__codelineno-4-1></a><span class=c1># 导入必要的库</span>
<a id=__codelineno-4-2 name=__codelineno-4-2></a><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<a id=__codelineno-4-3 name=__codelineno-4-3></a><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras</span><span class=w> </span><span class=kn>import</span> <span class=n>layers</span><span class=p>,</span> <span class=n>models</span>
<a id=__codelineno-4-4 name=__codelineno-4-4></a>
<a id=__codelineno-4-5 name=__codelineno-4-5></a><span class=c1># 初始化一个顺序模型（前馈神经网络）</span>
<a id=__codelineno-4-6 name=__codelineno-4-6></a><span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>()</span>  <span class=c1># 初始化模型</span>
<a id=__codelineno-4-7 name=__codelineno-4-7></a>
<a id=__codelineno-4-8 name=__codelineno-4-8></a><span class=c1># ---- 卷积神经网络部分 ----</span>
<a id=__codelineno-4-9 name=__codelineno-4-9></a><span class=c1># 增加卷积运算层1，卷积核数量=32，卷积核大小3x3，激活函数relu</span>
<a id=__codelineno-4-10 name=__codelineno-4-10></a><span class=c1># input_shape=(32, 32, 3) 表示输入为32x32像素的彩色图片（3通道）</span>
<a id=__codelineno-4-11 name=__codelineno-4-11></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>)))</span>
<a id=__codelineno-4-12 name=__codelineno-4-12></a><span class=c1># 增加池化层1，池化窗口2x2，减少特征图尺寸</span>
<a id=__codelineno-4-13 name=__codelineno-4-13></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>MaxPooling2D</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>
<a id=__codelineno-4-14 name=__codelineno-4-14></a>
<a id=__codelineno-4-15 name=__codelineno-4-15></a><span class=c1># 增加卷积运算层2，卷积核数量=64，卷积核大小3x3，激活函数relu</span>
<a id=__codelineno-4-16 name=__codelineno-4-16></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>))</span>
<a id=__codelineno-4-17 name=__codelineno-4-17></a><span class=c1># 增加池化层2</span>
<a id=__codelineno-4-18 name=__codelineno-4-18></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>MaxPooling2D</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>
<a id=__codelineno-4-19 name=__codelineno-4-19></a>
<a id=__codelineno-4-20 name=__codelineno-4-20></a><span class=c1># 增加卷积运算层3，卷积核数量=64，卷积核大小3x3，激活函数relu</span>
<a id=__codelineno-4-21 name=__codelineno-4-21></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>))</span>
<a id=__codelineno-4-22 name=__codelineno-4-22></a>
<a id=__codelineno-4-23 name=__codelineno-4-23></a><span class=c1># ---- 全连接层（MLP）部分 ----</span>
<a id=__codelineno-4-24 name=__codelineno-4-24></a><span class=c1># 把多维特征图展平为一维向量，便于输入全连接层</span>
<a id=__codelineno-4-25 name=__codelineno-4-25></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Flatten</span><span class=p>())</span>
<a id=__codelineno-4-26 name=__codelineno-4-26></a><span class=c1># 增加全连接层（Dense），64个神经元，激活函数relu</span>
<a id=__codelineno-4-27 name=__codelineno-4-27></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>))</span>
<a id=__codelineno-4-28 name=__codelineno-4-28></a><span class=c1># 增加输出层，10个神经元（如10分类），激活函数softmax用于多分类输出</span>
<a id=__codelineno-4-29 name=__codelineno-4-29></a><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>'softmax'</span><span class=p>))</span>
<a id=__codelineno-4-30 name=__codelineno-4-30></a>
<a id=__codelineno-4-31 name=__codelineno-4-31></a><span class=c1># ---- 模型汇总与编译 ----</span>
<a id=__codelineno-4-32 name=__codelineno-4-32></a><span class=c1># 输出模型结构信息</span>
<a id=__codelineno-4-33 name=__codelineno-4-33></a><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
<a id=__codelineno-4-34 name=__codelineno-4-34></a>
<a id=__codelineno-4-35 name=__codelineno-4-35></a><span class=c1># 编译模型，指定损失函数、优化器和评估指标</span>
<a id=__codelineno-4-36 name=__codelineno-4-36></a><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>'adam'</span><span class=p>,</span>                <span class=c1># 优化器</span>
<a id=__codelineno-4-37 name=__codelineno-4-37></a>              <span class=n>loss</span><span class=o>=</span><span class=s1>'sparse_categorical_crossentropy'</span><span class=p>,</span> <span class=c1># 损失函数（适合多分类标签为整数的情况）(稀疏分类交叉熵)</span>
<a id=__codelineno-4-38 name=__codelineno-4-38></a>              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>'accuracy'</span><span class=p>])</span>            <span class=c1># 评估指标</span>
<a id=__codelineno-4-39 name=__codelineno-4-39></a>
<a id=__codelineno-4-40 name=__codelineno-4-40></a><span class=c1># 你的模型已经准备好，可以用 model.fit() 进行训练</span>
<a id=__codelineno-4-41 name=__codelineno-4-41></a><span class=c1># 例如：</span>
<a id=__codelineno-4-42 name=__codelineno-4-42></a><span class=c1># model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))</span>
</code></pre></div></td></tr></tbody></table></div> <div class=highlight><table class=highlighttable><tbody><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-5-1> 1</a></span>
<span class=normal><a href=#__codelineno-5-2> 2</a></span>
<span class=normal><a href=#__codelineno-5-3> 3</a></span>
<span class=normal><a href=#__codelineno-5-4> 4</a></span>
<span class=normal><a href=#__codelineno-5-5> 5</a></span>
<span class=normal><a href=#__codelineno-5-6> 6</a></span>
<span class=normal><a href=#__codelineno-5-7> 7</a></span>
<span class=normal><a href=#__codelineno-5-8> 8</a></span>
<span class=normal><a href=#__codelineno-5-9> 9</a></span>
<span class=normal><a href=#__codelineno-5-10>10</a></span>
<span class=normal><a href=#__codelineno-5-11>11</a></span>
<span class=normal><a href=#__codelineno-5-12>12</a></span>
<span class=normal><a href=#__codelineno-5-13>13</a></span>
<span class=normal><a href=#__codelineno-5-14>14</a></span>
<span class=normal><a href=#__codelineno-5-15>15</a></span>
<span class=normal><a href=#__codelineno-5-16>16</a></span>
<span class=normal><a href=#__codelineno-5-17>17</a></span>
<span class=normal><a href=#__codelineno-5-18>18</a></span>
<span class=normal><a href=#__codelineno-5-19>19</a></span>
<span class=normal><a href=#__codelineno-5-20>20</a></span>
<span class=normal><a href=#__codelineno-5-21>21</a></span>
<span class=normal><a href=#__codelineno-5-22>22</a></span>
<span class=normal><a href=#__codelineno-5-23>23</a></span>
<span class=normal><a href=#__codelineno-5-24>24</a></span>
<span class=normal><a href=#__codelineno-5-25>25</a></span>
<span class=normal><a href=#__codelineno-5-26>26</a></span>
<span class=normal><a href=#__codelineno-5-27>27</a></span>
<span class=normal><a href=#__codelineno-5-28>28</a></span>
<span class=normal><a href=#__codelineno-5-29>29</a></span>
<span class=normal><a href=#__codelineno-5-30>30</a></span>
<span class=normal><a href=#__codelineno-5-31>31</a></span>
<span class=normal><a href=#__codelineno-5-32>32</a></span>
<span class=normal><a href=#__codelineno-5-33>33</a></span>
<span class=normal><a href=#__codelineno-5-34>34</a></span>
<span class=normal><a href=#__codelineno-5-35>35</a></span>
<span class=normal><a href=#__codelineno-5-36>36</a></span>
<span class=normal><a href=#__codelineno-5-37>37</a></span>
<span class=normal><a href=#__codelineno-5-38>38</a></span>
<span class=normal><a href=#__codelineno-5-39>39</a></span>
<span class=normal><a href=#__codelineno-5-40>40</a></span>
<span class=normal><a href=#__codelineno-5-41>41</a></span>
<span class=normal><a href=#__codelineno-5-42>42</a></span>
<span class=normal><a href=#__codelineno-5-43>43</a></span>
<span class=normal><a href=#__codelineno-5-44>44</a></span>
<span class=normal><a href=#__codelineno-5-45>45</a></span>
<span class=normal><a href=#__codelineno-5-46>46</a></span>
<span class=normal><a href=#__codelineno-5-47>47</a></span>
<span class=normal><a href=#__codelineno-5-48>48</a></span>
<span class=normal><a href=#__codelineno-5-49>49</a></span>
<span class=normal><a href=#__codelineno-5-50>50</a></span>
<span class=normal><a href=#__codelineno-5-51>51</a></span>
<span class=normal><a href=#__codelineno-5-52>52</a></span>
<span class=normal><a href=#__codelineno-5-53>53</a></span>
<span class=normal><a href=#__codelineno-5-54>54</a></span>
<span class=normal><a href=#__codelineno-5-55>55</a></span>
<span class=normal><a href=#__codelineno-5-56>56</a></span>
<span class=normal><a href=#__codelineno-5-57>57</a></span>
<span class=normal><a href=#__codelineno-5-58>58</a></span>
<span class=normal><a href=#__codelineno-5-59>59</a></span>
<span class=normal><a href=#__codelineno-5-60>60</a></span>
<span class=normal><a href=#__codelineno-5-61>61</a></span>
<span class=normal><a href=#__codelineno-5-62>62</a></span>
<span class=normal><a href=#__codelineno-5-63>63</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-5-1 name=__codelineno-5-1></a><span class=c1># 导入必要的库</span>
<a id=__codelineno-5-2 name=__codelineno-5-2></a><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.models</span><span class=w> </span><span class=kn>import</span> <span class=n>Sequential</span>
<a id=__codelineno-5-3 name=__codelineno-5-3></a><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.layers</span><span class=w> </span><span class=kn>import</span> <span class=n>LSTM</span><span class=p>,</span> <span class=n>Dense</span>
<a id=__codelineno-5-4 name=__codelineno-5-4></a>
<a id=__codelineno-5-5 name=__codelineno-5-5></a><span class=k>def</span><span class=w> </span><span class=nf>model1</span><span class=p>():</span>
<a id=__codelineno-5-6 name=__codelineno-5-6></a>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>  <span class=c1># 初始化一个顺序模型</span>
<a id=__codelineno-5-7 name=__codelineno-5-7></a>
<a id=__codelineno-5-8 name=__codelineno-5-8></a>    <span class=c1># 第一层 LSTM，128 单元，激活函数 relu，输入形状 (n_input, 1)</span>
<a id=__codelineno-5-9 name=__codelineno-5-9></a>    <span class=c1># dropout 和 recurrent_dropout 都为 0.5，用于防止过拟合(表示每个时间步输入的特征都有 0.5 的概率被屏蔽)</span>
<a id=__codelineno-5-10 name=__codelineno-5-10></a>    <span class=c1># return_sequences=True 表示输出整个序列，为后续 LSTM 层做输入</span>
<a id=__codelineno-5-11 name=__codelineno-5-11></a>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span>
<a id=__codelineno-5-12 name=__codelineno-5-12></a>        <span class=mi>128</span><span class=p>,</span> 
<a id=__codelineno-5-13 name=__codelineno-5-13></a>        <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>,</span> 
<a id=__codelineno-5-14 name=__codelineno-5-14></a>        <span class=n>dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
<a id=__codelineno-5-15 name=__codelineno-5-15></a>        <span class=n>recurrent_dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span>
<a id=__codelineno-5-16 name=__codelineno-5-16></a>        <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>n_input</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
<a id=__codelineno-5-17 name=__codelineno-5-17></a>        <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span>
<a id=__codelineno-5-18 name=__codelineno-5-18></a>    <span class=p>))</span>
<a id=__codelineno-5-19 name=__codelineno-5-19></a>
<a id=__codelineno-5-20 name=__codelineno-5-20></a>    <span class=c1># 第二层 LSTM，64 单元，同样设置 dropout，输出序列</span>
<a id=__codelineno-5-21 name=__codelineno-5-21></a>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span>
<a id=__codelineno-5-22 name=__codelineno-5-22></a>        <span class=mi>64</span><span class=p>,</span> 
<a id=__codelineno-5-23 name=__codelineno-5-23></a>        <span class=n>dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
<a id=__codelineno-5-24 name=__codelineno-5-24></a>        <span class=n>recurrent_dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
<a id=__codelineno-5-25 name=__codelineno-5-25></a>        <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>,</span> 
<a id=__codelineno-5-26 name=__codelineno-5-26></a>        <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span>
<a id=__codelineno-5-27 name=__codelineno-5-27></a>    <span class=p>))</span>
<a id=__codelineno-5-28 name=__codelineno-5-28></a>
<a id=__codelineno-5-29 name=__codelineno-5-29></a>    <span class=c1># 第三层 LSTM，只用 1 个单元（通常用于输出最后一个序列特征）</span>
<a id=__codelineno-5-30 name=__codelineno-5-30></a>    <span class=c1># return_sequences=False 表示只输出最后一个时间步的结果</span>
<a id=__codelineno-5-31 name=__codelineno-5-31></a>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span>
<a id=__codelineno-5-32 name=__codelineno-5-32></a>        <span class=mi>1</span><span class=p>,</span> 
<a id=__codelineno-5-33 name=__codelineno-5-33></a>        <span class=n>dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
<a id=__codelineno-5-34 name=__codelineno-5-34></a>        <span class=n>recurrent_dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
<a id=__codelineno-5-35 name=__codelineno-5-35></a>        <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>,</span> 
<a id=__codelineno-5-36 name=__codelineno-5-36></a>        <span class=n>return_sequences</span><span class=o>=</span><span class=kc>False</span>
<a id=__codelineno-5-37 name=__codelineno-5-37></a>    <span class=p>))</span>
<a id=__codelineno-5-38 name=__codelineno-5-38></a>
<a id=__codelineno-5-39 name=__codelineno-5-39></a>    <span class=c1># 添加一个全连接层（Dense），用于输出预测结果</span>
<a id=__codelineno-5-40 name=__codelineno-5-40></a>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
<a id=__codelineno-5-41 name=__codelineno-5-41></a>
<a id=__codelineno-5-42 name=__codelineno-5-42></a>    <span class=c1># 编译模型，优化器为 adam，损失函数为均方误差 mse，评估指标为 accuracy</span>
<a id=__codelineno-5-43 name=__codelineno-5-43></a>    <span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>'adam'</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>'mse'</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>'accuracy'</span><span class=p>])</span>
<a id=__codelineno-5-44 name=__codelineno-5-44></a>
<a id=__codelineno-5-45 name=__codelineno-5-45></a>    <span class=k>return</span> <span class=n>model</span>
<a id=__codelineno-5-46 name=__codelineno-5-46></a>
<a id=__codelineno-5-47 name=__codelineno-5-47></a><span class=k>def</span><span class=w> </span><span class=nf>model2</span><span class=p>():</span>
<a id=__codelineno-5-48 name=__codelineno-5-48></a>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>  <span class=c1># 初始化模型</span>
<a id=__codelineno-5-49 name=__codelineno-5-49></a>
<a id=__codelineno-5-50 name=__codelineno-5-50></a>    <span class=c1># 只有一层 LSTM，50 单元，激活函数 relu，输入形状 (n_input, 1)</span>
<a id=__codelineno-5-51 name=__codelineno-5-51></a>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span>
<a id=__codelineno-5-52 name=__codelineno-5-52></a>        <span class=mi>50</span><span class=p>,</span> 
<a id=__codelineno-5-53 name=__codelineno-5-53></a>        <span class=n>activation</span><span class=o>=</span><span class=s1>'relu'</span><span class=p>,</span> 
<a id=__codelineno-5-54 name=__codelineno-5-54></a>        <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>n_input</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<a id=__codelineno-5-55 name=__codelineno-5-55></a>    <span class=p>))</span>
<a id=__codelineno-5-56 name=__codelineno-5-56></a>
<a id=__codelineno-5-57 name=__codelineno-5-57></a>    <span class=c1># 添加全连接层，输出预测值</span>
<a id=__codelineno-5-58 name=__codelineno-5-58></a>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
<a id=__codelineno-5-59 name=__codelineno-5-59></a>
<a id=__codelineno-5-60 name=__codelineno-5-60></a>    <span class=c1># 编译模型</span>
<a id=__codelineno-5-61 name=__codelineno-5-61></a>    <span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>'adam'</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>'mse'</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>'accuracy'</span><span class=p>])</span>
<a id=__codelineno-5-62 name=__codelineno-5-62></a>
<a id=__codelineno-5-63 name=__codelineno-5-63></a>    <span class=k>return</span> <span class=n>model</span>
</code></pre></div></td></tr></tbody></table></div> <h2 id=10-ii>10. 附则 II 补充相关知识<a class=headerlink href=#10-ii title="Permanent link">#</a></h2> <hr> <p>GNN: 生成式对抗网络. 其思想是让图灵测试自动化, 让机器人判断对方是不是机器人. 其训练方式是同时投入真数据和伪装数据给判断方, 让判断方判别真假, 并逐步增强其辨别真假的能力; 同时让生成方不断生成更逼真的伪装数据. 结束条件为达到纳什均衡(0.5).</p> <div class=footnote> <hr> <ol> <li id=fn:1> <p>AlexNet (2012), The input to the network is a 224×224 RGB image. <a href=https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf>https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>&nbsp;<a class=footnote-backref href=#fnref:1 title="Jump back to footnote 1 in the text">↩</a></p> </li> </ol> </div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="September 15, 2025 01:54:15 UTC">September 15, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="June 18, 2025 13:43:33 UTC">June 18, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Contributors> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"></path></svg> </span> <nav> <a href=mailto:2901448780@qq.com>HollowDobt</a> </nav> </span> <span class=md-source-file__fact> <span class=md-icon title=Contributors> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path></svg> </span> <span>GitHub</span> <nav> <a href=https://github.com/HollowDobt class=md-author title=@HollowDobt> <img src="https://avatars.githubusercontent.com/u/197474030?v=4&size=72" alt=HollowDobt> </a> </nav> </span> </aside> <h2 id=__comments>Comments</h2> <script src=https://giscus.app/client.js data-repo=HollowDobt/docs-site data-repo-id=R_kgDOOvKsgw data-category=Comments data-category-id=DIC_kwDOOvKsg84CqrwA data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=https://docs.hollowlib.top/_stylesheets/comments.css data-lang=en data-loading=lazy crossorigin=anonymous async>
  </script> <script>
    var giscus = document.querySelector("script[src*=giscus]");
    var palette = __md_get("__palette");
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate" ? "transparent_dark" : "light";
      giscus.setAttribute("data-theme", theme);
    }

    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]");
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette");
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "transparent_dark" : "light";
          var frame = document.querySelector(".giscus-frame");
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          );
        }
      });
    });
  </script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../NorCheT%28B%29/ class="md-footer__link md-footer__link--prev" aria-label="Previous: 普通化学实验(乙)"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 普通化学实验(乙) </div> </div> </a> <a href=../Electron/ class="md-footer__link md-footer__link--next" aria-label="Next: 电工电子学"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 电工电子学 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> © 2025 - today Hollow Dobt | <a href="https://icp.gov.moe/?keyword=20257760" target=_blank>萌ICP备20257760号</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/hollowdobt target=_blank rel=noopener title="Hollow's Github" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> <a href=mailto:rxq@hollowlib.top target=_blank rel=noopener title="Hollow's Email" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l208 156a48 48 0 0 0 57.6 0l208-156c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48zM0 196v188c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V196L313.6 344.8c-34.1 25.6-81.1 25.6-115.2 0z"></path></svg> </a> <a href=https://hollowlib.top target=_blank rel=noopener title="Hollow's Main Site" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="m241 87.1 15 20.7 15-20.7C296 52.5 336.2 32 378.9 32 452.4 32 512 91.6 512 165.1v2.6c0 112.2-139.9 242.5-212.9 298.2-12.4 9.4-27.6 14.1-43.1 14.1s-30.8-4.6-43.1-14.1C139.9 410.2 0 279.9 0 167.7v-2.6C0 91.6 59.6 32 133.1 32 175.8 32 216 52.5 241 87.1"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.footer", "navigation.indexes", "navigation.tracking", "naviagtion.prune", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.preview", "search.suggest", "search.highlight", "search.share", "content.tabs.link", "content.tooltips", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "toc.follow"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../../_javascripts/math.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>